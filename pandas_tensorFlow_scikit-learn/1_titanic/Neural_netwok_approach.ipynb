{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test.csv\n",
      "./train.csv\n",
      "./gender_submission.csv\n",
      "./model_predict.ipynb\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import relu,sigmoid\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the training set are (891, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dimensions of the training set are {train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features?\n",
    "Age, Fare, Sex (hot-encoded value), Engineered Title (hot-encoded value), Pclass, Sibsp, Parch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  SibSp  Parch     Fare  Sex_female  Sex_male\n",
       "PassengerId                                                           \n",
       "1                 3  22.0      1      0   7.2500           0         1\n",
       "2                 1  38.0      1      0  71.2833           1         0\n",
       "3                 3  26.0      0      0   7.9250           1         0\n",
       "4                 1  35.0      1      0  53.1000           1         0\n",
       "5                 3  35.0      0      0   8.0500           0         1"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import again to ensure stable testing\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "\n",
    "# drop irrelevant columns and fill NaN age with average\n",
    "def drop_irrev_col(df):\n",
    "    # change index\n",
    "    df.set_index('PassengerId',inplace=True)\n",
    "    # drop some irrelevant features\n",
    "    df.drop(['Ticket', 'Cabin','Embarked'], axis=1, inplace=True)\n",
    "    # fill NaN age with average\n",
    "    df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # RegEx to extract titles and print them\n",
    "    #df['Title'] = df['Name'].str.extract(r'(\\w+)(?=\\.)')\n",
    "    #if(df.Title.isnull().sum()!=0):\n",
    "    #    warnings.warn(\"NaN values are found in Titles. Fix before moving on.\")\n",
    "    #print(f\"Titles found are {df.Title.unique()}\")\n",
    "    df.drop('Name', axis=1, inplace=True)\n",
    "    # one-hot encode sex and titles\n",
    "    #df_new = pd.get_dummies(df, columns=['Sex','Title'],dtype=int)\n",
    "    df_new = pd.get_dummies(df, columns=['Sex'],dtype=int)\n",
    "    return df_new\n",
    "\n",
    "# separate the training set into features dataframe and target data series\n",
    "def features_and_target(df):\n",
    "    # everything but survived is a feature\n",
    "    features = [i for i in df.columns.tolist() if i!= 'Survived']\n",
    "    X = df [features]\n",
    "    # binary target\n",
    "    Y = df['Survived']\n",
    "    return X, Y\n",
    "\n",
    "# clean the data\n",
    "clean_train_data = drop_irrev_col(train_data)\n",
    "clean_train_data = feature_engineering(clean_train_data)\n",
    "# separate features and targets\n",
    "X_train, y_train = features_and_target(clean_train_data)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 891 entries, 1 to 891\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Pclass      891 non-null    int64  \n",
      " 1   Age         891 non-null    float64\n",
      " 2   SibSp       891 non-null    int64  \n",
      " 3   Parch       891 non-null    int64  \n",
      " 4   Fare        891 non-null    float64\n",
      " 5   Sex_female  891 non-null    int64  \n",
      " 6   Sex_male    891 non-null    int64  \n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 55.7 KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "Index: 891 entries, 1 to 891\n",
      "Series name: Survived\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "891 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 13.9 KB\n"
     ]
    }
   ],
   "source": [
    "y_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Age?\n",
    "Let's try without first and see diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " L_i (Dense)                 (None, 16)                128       \n",
      "                                                                 \n",
      " L_ii (Dense)                (None, 10)                170       \n",
      "                                                                 \n",
      " L_iii (Dense)               (None, 8)                 88        \n",
      "                                                                 \n",
      " L_iv (Dense)                (None, 3)                 27        \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 417 (1.63 KB)\n",
      "Trainable params: 417 (1.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(16, activation='relu', name=\"L_i\"),\n",
    "        Dense(10, activation='relu',kernel_regularizer=l2(0.05), name= \"L_ii\"),\n",
    "        Dense(8, activation='relu',name= \"L_iii\"),\n",
    "        Dense(3, activation='relu', kernel_regularizer=l2(0.05),name= \"L_iv\"),\n",
    "        Dense(1, activation='linear', name=\"Output\")\n",
    "\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=Adam(0.0001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# build the model and specify input dimensions\n",
    "model.build(input_shape=(None, X_train.shape[1]))\n",
    "# Store the initial weights\n",
    "initial_weights = model.get_weights()\n",
    "\n",
    "# Function to reset to initial weights\n",
    "def reset_to_initial_weights(model, initial_weights):\n",
    "    model.set_weights(initial_weights)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "178/178 [==============================] - 1s 2ms/step - loss: 1.8450 - accuracy: 0.6419 - val_loss: 1.5299 - val_accuracy: 0.6927\n",
      "Epoch 2/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.5413 - accuracy: 0.6601 - val_loss: 1.4136 - val_accuracy: 0.7039\n",
      "Epoch 3/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.4181 - accuracy: 0.6545 - val_loss: 1.3888 - val_accuracy: 0.6592\n",
      "Epoch 4/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.3733 - accuracy: 0.6180 - val_loss: 1.3431 - val_accuracy: 0.6704\n",
      "Epoch 5/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.3297 - accuracy: 0.6208 - val_loss: 1.3054 - val_accuracy: 0.6536\n",
      "Epoch 6/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.2885 - accuracy: 0.6208 - val_loss: 1.2649 - val_accuracy: 0.6592\n",
      "Epoch 7/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 1.2493 - accuracy: 0.6292 - val_loss: 1.2253 - val_accuracy: 0.6648\n",
      "Epoch 8/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.2127 - accuracy: 0.6292 - val_loss: 1.1865 - val_accuracy: 0.6704\n",
      "Epoch 9/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 1.1807 - accuracy: 0.6362 - val_loss: 1.1554 - val_accuracy: 0.6648\n",
      "Epoch 10/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1480 - accuracy: 0.6419 - val_loss: 1.1319 - val_accuracy: 0.6480\n",
      "Epoch 11/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.1182 - accuracy: 0.6320 - val_loss: 1.0967 - val_accuracy: 0.6704\n",
      "Epoch 12/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.0906 - accuracy: 0.6306 - val_loss: 1.0683 - val_accuracy: 0.6704\n",
      "Epoch 13/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 1.0647 - accuracy: 0.6433 - val_loss: 1.0454 - val_accuracy: 0.6704\n",
      "Epoch 14/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 1.0397 - accuracy: 0.6433 - val_loss: 1.0219 - val_accuracy: 0.6704\n",
      "Epoch 15/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 1.0175 - accuracy: 0.6433 - val_loss: 0.9966 - val_accuracy: 0.6983\n",
      "Epoch 16/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.9987 - accuracy: 0.6461 - val_loss: 0.9792 - val_accuracy: 0.6760\n",
      "Epoch 17/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.9784 - accuracy: 0.6489 - val_loss: 0.9637 - val_accuracy: 0.6648\n",
      "Epoch 18/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.9602 - accuracy: 0.6503 - val_loss: 0.9419 - val_accuracy: 0.6872\n",
      "Epoch 19/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.9452 - accuracy: 0.6433 - val_loss: 0.9271 - val_accuracy: 0.6760\n",
      "Epoch 20/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.9286 - accuracy: 0.6419 - val_loss: 0.9114 - val_accuracy: 0.6872\n",
      "Epoch 21/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.9137 - accuracy: 0.6503 - val_loss: 0.8955 - val_accuracy: 0.6872\n",
      "Epoch 22/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.9008 - accuracy: 0.6489 - val_loss: 0.8896 - val_accuracy: 0.6648\n",
      "Epoch 23/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8892 - accuracy: 0.6419 - val_loss: 0.8716 - val_accuracy: 0.6816\n",
      "Epoch 24/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8771 - accuracy: 0.6559 - val_loss: 0.8687 - val_accuracy: 0.6648\n",
      "Epoch 25/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8687 - accuracy: 0.6404 - val_loss: 0.8512 - val_accuracy: 0.6872\n",
      "Epoch 26/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8580 - accuracy: 0.6545 - val_loss: 0.8414 - val_accuracy: 0.6983\n",
      "Epoch 27/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8507 - accuracy: 0.6503 - val_loss: 0.8341 - val_accuracy: 0.6983\n",
      "Epoch 28/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8427 - accuracy: 0.6615 - val_loss: 0.8315 - val_accuracy: 0.6872\n",
      "Epoch 29/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8378 - accuracy: 0.6517 - val_loss: 0.8227 - val_accuracy: 0.6872\n",
      "Epoch 30/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8297 - accuracy: 0.6615 - val_loss: 0.8152 - val_accuracy: 0.6983\n",
      "Epoch 31/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8236 - accuracy: 0.6559 - val_loss: 0.8088 - val_accuracy: 0.6927\n",
      "Epoch 32/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8182 - accuracy: 0.6545 - val_loss: 0.7984 - val_accuracy: 0.7095\n",
      "Epoch 33/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.8158 - accuracy: 0.6657 - val_loss: 0.8001 - val_accuracy: 0.6872\n",
      "Epoch 34/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8092 - accuracy: 0.6615 - val_loss: 0.7930 - val_accuracy: 0.6983\n",
      "Epoch 35/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8065 - accuracy: 0.6615 - val_loss: 0.7943 - val_accuracy: 0.6872\n",
      "Epoch 36/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.8015 - accuracy: 0.6615 - val_loss: 0.7836 - val_accuracy: 0.7039\n",
      "Epoch 37/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7993 - accuracy: 0.6713 - val_loss: 0.7838 - val_accuracy: 0.6872\n",
      "Epoch 38/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7959 - accuracy: 0.6643 - val_loss: 0.7801 - val_accuracy: 0.6927\n",
      "Epoch 39/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7914 - accuracy: 0.6629 - val_loss: 0.7778 - val_accuracy: 0.6872\n",
      "Epoch 40/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7879 - accuracy: 0.6643 - val_loss: 0.7755 - val_accuracy: 0.6872\n",
      "Epoch 41/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7874 - accuracy: 0.6559 - val_loss: 0.7699 - val_accuracy: 0.6983\n",
      "Epoch 42/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7838 - accuracy: 0.6629 - val_loss: 0.7679 - val_accuracy: 0.6983\n",
      "Epoch 43/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7814 - accuracy: 0.6629 - val_loss: 0.7678 - val_accuracy: 0.6872\n",
      "Epoch 44/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7785 - accuracy: 0.6728 - val_loss: 0.7623 - val_accuracy: 0.6983\n",
      "Epoch 45/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7749 - accuracy: 0.6770 - val_loss: 0.7629 - val_accuracy: 0.6872\n",
      "Epoch 46/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7731 - accuracy: 0.6643 - val_loss: 0.7549 - val_accuracy: 0.6927\n",
      "Epoch 47/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7694 - accuracy: 0.6629 - val_loss: 0.7501 - val_accuracy: 0.6983\n",
      "Epoch 48/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7665 - accuracy: 0.6699 - val_loss: 0.7464 - val_accuracy: 0.6927\n",
      "Epoch 49/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7626 - accuracy: 0.6643 - val_loss: 0.7393 - val_accuracy: 0.7151\n",
      "Epoch 50/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7597 - accuracy: 0.6713 - val_loss: 0.7358 - val_accuracy: 0.6983\n",
      "Epoch 51/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7553 - accuracy: 0.6713 - val_loss: 0.7303 - val_accuracy: 0.7039\n",
      "Epoch 52/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7514 - accuracy: 0.6643 - val_loss: 0.7190 - val_accuracy: 0.7207\n",
      "Epoch 53/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7503 - accuracy: 0.6699 - val_loss: 0.7226 - val_accuracy: 0.6927\n",
      "Epoch 54/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7472 - accuracy: 0.6728 - val_loss: 0.7184 - val_accuracy: 0.6927\n",
      "Epoch 55/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7417 - accuracy: 0.6770 - val_loss: 0.7218 - val_accuracy: 0.6927\n",
      "Epoch 56/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7410 - accuracy: 0.6615 - val_loss: 0.6978 - val_accuracy: 0.7263\n",
      "Epoch 57/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7342 - accuracy: 0.6812 - val_loss: 0.7074 - val_accuracy: 0.6927\n",
      "Epoch 58/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7358 - accuracy: 0.6685 - val_loss: 0.6918 - val_accuracy: 0.7263\n",
      "Epoch 59/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7338 - accuracy: 0.6770 - val_loss: 0.6922 - val_accuracy: 0.7095\n",
      "Epoch 60/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7295 - accuracy: 0.6784 - val_loss: 0.6835 - val_accuracy: 0.7151\n",
      "Epoch 61/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7267 - accuracy: 0.6812 - val_loss: 0.6773 - val_accuracy: 0.7207\n",
      "Epoch 62/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7254 - accuracy: 0.6770 - val_loss: 0.6750 - val_accuracy: 0.7374\n",
      "Epoch 63/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7227 - accuracy: 0.6798 - val_loss: 0.6751 - val_accuracy: 0.7151\n",
      "Epoch 64/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7211 - accuracy: 0.6854 - val_loss: 0.6742 - val_accuracy: 0.7151\n",
      "Epoch 65/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7214 - accuracy: 0.6728 - val_loss: 0.6674 - val_accuracy: 0.7263\n",
      "Epoch 66/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7189 - accuracy: 0.6826 - val_loss: 0.6727 - val_accuracy: 0.7095\n",
      "Epoch 67/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7148 - accuracy: 0.6840 - val_loss: 0.6685 - val_accuracy: 0.7151\n",
      "Epoch 68/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7160 - accuracy: 0.6840 - val_loss: 0.6641 - val_accuracy: 0.7263\n",
      "Epoch 69/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7137 - accuracy: 0.6784 - val_loss: 0.6588 - val_accuracy: 0.7318\n",
      "Epoch 70/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7126 - accuracy: 0.6854 - val_loss: 0.6583 - val_accuracy: 0.7263\n",
      "Epoch 71/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7113 - accuracy: 0.6854 - val_loss: 0.6594 - val_accuracy: 0.7207\n",
      "Epoch 72/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7095 - accuracy: 0.6896 - val_loss: 0.6615 - val_accuracy: 0.7151\n",
      "Epoch 73/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7061 - accuracy: 0.6854 - val_loss: 0.6594 - val_accuracy: 0.7151\n",
      "Epoch 74/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7074 - accuracy: 0.6812 - val_loss: 0.6578 - val_accuracy: 0.7151\n",
      "Epoch 75/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.7042 - accuracy: 0.6812 - val_loss: 0.6561 - val_accuracy: 0.7151\n",
      "Epoch 76/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7030 - accuracy: 0.6854 - val_loss: 0.6495 - val_accuracy: 0.7318\n",
      "Epoch 77/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7008 - accuracy: 0.6924 - val_loss: 0.6473 - val_accuracy: 0.7318\n",
      "Epoch 78/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.7000 - accuracy: 0.6854 - val_loss: 0.6479 - val_accuracy: 0.7207\n",
      "Epoch 79/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6963 - accuracy: 0.6910 - val_loss: 0.6553 - val_accuracy: 0.7207\n",
      "Epoch 80/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6975 - accuracy: 0.6798 - val_loss: 0.6432 - val_accuracy: 0.7318\n",
      "Epoch 81/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6950 - accuracy: 0.6840 - val_loss: 0.6420 - val_accuracy: 0.7263\n",
      "Epoch 82/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.6868 - val_loss: 0.6354 - val_accuracy: 0.7430\n",
      "Epoch 83/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.6896 - val_loss: 0.6332 - val_accuracy: 0.7374\n",
      "Epoch 84/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6896 - accuracy: 0.6924 - val_loss: 0.6488 - val_accuracy: 0.7263\n",
      "Epoch 85/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6897 - accuracy: 0.6882 - val_loss: 0.6348 - val_accuracy: 0.7430\n",
      "Epoch 86/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6872 - accuracy: 0.6868 - val_loss: 0.6369 - val_accuracy: 0.7374\n",
      "Epoch 87/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.6896 - val_loss: 0.6332 - val_accuracy: 0.7374\n",
      "Epoch 88/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.6910 - val_loss: 0.6293 - val_accuracy: 0.7486\n",
      "Epoch 89/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6812 - accuracy: 0.6938 - val_loss: 0.6219 - val_accuracy: 0.7486\n",
      "Epoch 90/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6813 - accuracy: 0.6994 - val_loss: 0.6277 - val_accuracy: 0.7430\n",
      "Epoch 91/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6774 - accuracy: 0.6966 - val_loss: 0.6193 - val_accuracy: 0.7542\n",
      "Epoch 92/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6764 - accuracy: 0.7008 - val_loss: 0.6233 - val_accuracy: 0.7430\n",
      "Epoch 93/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6718 - accuracy: 0.6966 - val_loss: 0.6162 - val_accuracy: 0.7542\n",
      "Epoch 94/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6717 - accuracy: 0.7022 - val_loss: 0.6135 - val_accuracy: 0.7542\n",
      "Epoch 95/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6709 - accuracy: 0.7008 - val_loss: 0.6105 - val_accuracy: 0.7598\n",
      "Epoch 96/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6678 - accuracy: 0.7135 - val_loss: 0.6226 - val_accuracy: 0.7486\n",
      "Epoch 97/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6668 - accuracy: 0.7065 - val_loss: 0.6086 - val_accuracy: 0.7654\n",
      "Epoch 98/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6657 - accuracy: 0.7065 - val_loss: 0.6078 - val_accuracy: 0.7654\n",
      "Epoch 99/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6621 - accuracy: 0.7149 - val_loss: 0.6060 - val_accuracy: 0.7654\n",
      "Epoch 100/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6590 - accuracy: 0.7149 - val_loss: 0.6014 - val_accuracy: 0.7709\n",
      "Epoch 101/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6577 - accuracy: 0.7065 - val_loss: 0.5973 - val_accuracy: 0.7654\n",
      "Epoch 102/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6554 - accuracy: 0.7135 - val_loss: 0.5948 - val_accuracy: 0.7765\n",
      "Epoch 103/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6561 - accuracy: 0.7093 - val_loss: 0.5927 - val_accuracy: 0.7654\n",
      "Epoch 104/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6478 - accuracy: 0.7177 - val_loss: 0.5919 - val_accuracy: 0.7821\n",
      "Epoch 105/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6489 - accuracy: 0.7079 - val_loss: 0.5978 - val_accuracy: 0.7654\n",
      "Epoch 106/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.7149 - val_loss: 0.6087 - val_accuracy: 0.7654\n",
      "Epoch 107/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6440 - accuracy: 0.7177 - val_loss: 0.5850 - val_accuracy: 0.7765\n",
      "Epoch 108/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6407 - accuracy: 0.7191 - val_loss: 0.5823 - val_accuracy: 0.7765\n",
      "Epoch 109/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6360 - accuracy: 0.7219 - val_loss: 0.5799 - val_accuracy: 0.7765\n",
      "Epoch 110/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6367 - accuracy: 0.7261 - val_loss: 0.5767 - val_accuracy: 0.7765\n",
      "Epoch 111/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6320 - accuracy: 0.7261 - val_loss: 0.5729 - val_accuracy: 0.7765\n",
      "Epoch 112/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6293 - accuracy: 0.7317 - val_loss: 0.5775 - val_accuracy: 0.7765\n",
      "Epoch 113/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6249 - accuracy: 0.7247 - val_loss: 0.5922 - val_accuracy: 0.7654\n",
      "Epoch 114/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6256 - accuracy: 0.7261 - val_loss: 0.5634 - val_accuracy: 0.7821\n",
      "Epoch 115/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6219 - accuracy: 0.7275 - val_loss: 0.5632 - val_accuracy: 0.7821\n",
      "Epoch 116/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6199 - accuracy: 0.7275 - val_loss: 0.5620 - val_accuracy: 0.7821\n",
      "Epoch 117/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6174 - accuracy: 0.7374 - val_loss: 0.5577 - val_accuracy: 0.7821\n",
      "Epoch 118/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6165 - accuracy: 0.7374 - val_loss: 0.5615 - val_accuracy: 0.7709\n",
      "Epoch 119/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6148 - accuracy: 0.7346 - val_loss: 0.5547 - val_accuracy: 0.7765\n",
      "Epoch 120/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6132 - accuracy: 0.7402 - val_loss: 0.5529 - val_accuracy: 0.7765\n",
      "Epoch 121/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6089 - accuracy: 0.7346 - val_loss: 0.5471 - val_accuracy: 0.7933\n",
      "Epoch 122/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6069 - accuracy: 0.7374 - val_loss: 0.5448 - val_accuracy: 0.7933\n",
      "Epoch 123/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.6059 - accuracy: 0.7317 - val_loss: 0.5444 - val_accuracy: 0.7877\n",
      "Epoch 124/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6030 - accuracy: 0.7360 - val_loss: 0.5617 - val_accuracy: 0.7654\n",
      "Epoch 125/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6020 - accuracy: 0.7444 - val_loss: 0.5414 - val_accuracy: 0.7877\n",
      "Epoch 126/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5997 - accuracy: 0.7360 - val_loss: 0.5353 - val_accuracy: 0.7989\n",
      "Epoch 127/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.6006 - accuracy: 0.7416 - val_loss: 0.5446 - val_accuracy: 0.7765\n",
      "Epoch 128/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5965 - accuracy: 0.7402 - val_loss: 0.5412 - val_accuracy: 0.7821\n",
      "Epoch 129/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5922 - accuracy: 0.7472 - val_loss: 0.5305 - val_accuracy: 0.8045\n",
      "Epoch 130/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5945 - accuracy: 0.7402 - val_loss: 0.5339 - val_accuracy: 0.8101\n",
      "Epoch 131/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5931 - accuracy: 0.7458 - val_loss: 0.5292 - val_accuracy: 0.7989\n",
      "Epoch 132/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5978 - accuracy: 0.7444 - val_loss: 0.5239 - val_accuracy: 0.7933\n",
      "Epoch 133/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5913 - accuracy: 0.7458 - val_loss: 0.5254 - val_accuracy: 0.7989\n",
      "Epoch 134/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.7458 - val_loss: 0.5275 - val_accuracy: 0.7877\n",
      "Epoch 135/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5868 - accuracy: 0.7486 - val_loss: 0.5221 - val_accuracy: 0.8156\n",
      "Epoch 136/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5875 - accuracy: 0.7570 - val_loss: 0.5226 - val_accuracy: 0.8212\n",
      "Epoch 137/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5843 - accuracy: 0.7514 - val_loss: 0.5163 - val_accuracy: 0.8212\n",
      "Epoch 138/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5844 - accuracy: 0.7514 - val_loss: 0.5170 - val_accuracy: 0.8212\n",
      "Epoch 139/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.7542 - val_loss: 0.5231 - val_accuracy: 0.8101\n",
      "Epoch 140/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5829 - accuracy: 0.7584 - val_loss: 0.5215 - val_accuracy: 0.8101\n",
      "Epoch 141/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.7598 - val_loss: 0.5153 - val_accuracy: 0.8268\n",
      "Epoch 142/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7584 - val_loss: 0.5102 - val_accuracy: 0.8156\n",
      "Epoch 143/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5795 - accuracy: 0.7556 - val_loss: 0.5137 - val_accuracy: 0.8324\n",
      "Epoch 144/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.7542 - val_loss: 0.5093 - val_accuracy: 0.8268\n",
      "Epoch 145/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5766 - accuracy: 0.7584 - val_loss: 0.5189 - val_accuracy: 0.8045\n",
      "Epoch 146/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5757 - accuracy: 0.7640 - val_loss: 0.5108 - val_accuracy: 0.8212\n",
      "Epoch 147/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5747 - accuracy: 0.7654 - val_loss: 0.5119 - val_accuracy: 0.8268\n",
      "Epoch 148/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5746 - accuracy: 0.7654 - val_loss: 0.5068 - val_accuracy: 0.8268\n",
      "Epoch 149/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.7697 - val_loss: 0.5085 - val_accuracy: 0.8212\n",
      "Epoch 150/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.7683 - val_loss: 0.5077 - val_accuracy: 0.8324\n",
      "Epoch 151/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5710 - accuracy: 0.7725 - val_loss: 0.5129 - val_accuracy: 0.8212\n",
      "Epoch 152/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7654 - val_loss: 0.5064 - val_accuracy: 0.8212\n",
      "Epoch 153/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7640 - val_loss: 0.5111 - val_accuracy: 0.8212\n",
      "Epoch 154/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7683 - val_loss: 0.5048 - val_accuracy: 0.8324\n",
      "Epoch 155/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5696 - accuracy: 0.7711 - val_loss: 0.5046 - val_accuracy: 0.8268\n",
      "Epoch 156/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5691 - accuracy: 0.7683 - val_loss: 0.5004 - val_accuracy: 0.8324\n",
      "Epoch 157/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5674 - accuracy: 0.7725 - val_loss: 0.5009 - val_accuracy: 0.8324\n",
      "Epoch 158/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5672 - accuracy: 0.7669 - val_loss: 0.4988 - val_accuracy: 0.8324\n",
      "Epoch 159/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5651 - accuracy: 0.7697 - val_loss: 0.4982 - val_accuracy: 0.8268\n",
      "Epoch 160/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5658 - accuracy: 0.7739 - val_loss: 0.5009 - val_accuracy: 0.8268\n",
      "Epoch 161/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5652 - accuracy: 0.7711 - val_loss: 0.4978 - val_accuracy: 0.8268\n",
      "Epoch 162/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5641 - accuracy: 0.7683 - val_loss: 0.4984 - val_accuracy: 0.8268\n",
      "Epoch 163/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5627 - accuracy: 0.7767 - val_loss: 0.4961 - val_accuracy: 0.8268\n",
      "Epoch 164/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.7739 - val_loss: 0.4946 - val_accuracy: 0.8268\n",
      "Epoch 165/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7753 - val_loss: 0.4942 - val_accuracy: 0.8268\n",
      "Epoch 166/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5630 - accuracy: 0.7795 - val_loss: 0.4962 - val_accuracy: 0.8268\n",
      "Epoch 167/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5610 - accuracy: 0.7837 - val_loss: 0.4949 - val_accuracy: 0.8268\n",
      "Epoch 168/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5618 - accuracy: 0.7823 - val_loss: 0.4948 - val_accuracy: 0.8268\n",
      "Epoch 169/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.7823 - val_loss: 0.4941 - val_accuracy: 0.8212\n",
      "Epoch 170/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7739 - val_loss: 0.4926 - val_accuracy: 0.8212\n",
      "Epoch 171/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7809 - val_loss: 0.4887 - val_accuracy: 0.8268\n",
      "Epoch 172/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7711 - val_loss: 0.4907 - val_accuracy: 0.8268\n",
      "Epoch 173/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7809 - val_loss: 0.4918 - val_accuracy: 0.8268\n",
      "Epoch 174/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.7781 - val_loss: 0.4944 - val_accuracy: 0.8268\n",
      "Epoch 175/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5585 - accuracy: 0.7795 - val_loss: 0.4904 - val_accuracy: 0.8268\n",
      "Epoch 176/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.7823 - val_loss: 0.5001 - val_accuracy: 0.8156\n",
      "Epoch 177/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5567 - accuracy: 0.7795 - val_loss: 0.4952 - val_accuracy: 0.8212\n",
      "Epoch 178/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7809 - val_loss: 0.4901 - val_accuracy: 0.8268\n",
      "Epoch 179/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5558 - accuracy: 0.7851 - val_loss: 0.4906 - val_accuracy: 0.8324\n",
      "Epoch 180/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5565 - accuracy: 0.7711 - val_loss: 0.4973 - val_accuracy: 0.8212\n",
      "Epoch 181/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7767 - val_loss: 0.5001 - val_accuracy: 0.8156\n",
      "Epoch 182/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5538 - accuracy: 0.7809 - val_loss: 0.4918 - val_accuracy: 0.8212\n",
      "Epoch 183/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5547 - accuracy: 0.7851 - val_loss: 0.4859 - val_accuracy: 0.8268\n",
      "Epoch 184/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.7795 - val_loss: 0.4869 - val_accuracy: 0.8324\n",
      "Epoch 185/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5522 - accuracy: 0.7907 - val_loss: 0.4900 - val_accuracy: 0.8212\n",
      "Epoch 186/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.7767 - val_loss: 0.4854 - val_accuracy: 0.8268\n",
      "Epoch 187/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5511 - accuracy: 0.7837 - val_loss: 0.4988 - val_accuracy: 0.8156\n",
      "Epoch 188/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5518 - accuracy: 0.7697 - val_loss: 0.4881 - val_accuracy: 0.8380\n",
      "Epoch 189/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5530 - accuracy: 0.7809 - val_loss: 0.4826 - val_accuracy: 0.8268\n",
      "Epoch 190/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5511 - accuracy: 0.7865 - val_loss: 0.5082 - val_accuracy: 0.8101\n",
      "Epoch 191/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7795 - val_loss: 0.4836 - val_accuracy: 0.8268\n",
      "Epoch 192/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5505 - accuracy: 0.7809 - val_loss: 0.4833 - val_accuracy: 0.8324\n",
      "Epoch 193/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5502 - accuracy: 0.7837 - val_loss: 0.4846 - val_accuracy: 0.8324\n",
      "Epoch 194/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5534 - accuracy: 0.7865 - val_loss: 0.4878 - val_accuracy: 0.8268\n",
      "Epoch 195/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5485 - accuracy: 0.7865 - val_loss: 0.4852 - val_accuracy: 0.8324\n",
      "Epoch 196/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.7823 - val_loss: 0.4818 - val_accuracy: 0.8324\n",
      "Epoch 197/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.7837 - val_loss: 0.4818 - val_accuracy: 0.8324\n",
      "Epoch 198/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5496 - accuracy: 0.7865 - val_loss: 0.4831 - val_accuracy: 0.8324\n",
      "Epoch 199/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5464 - accuracy: 0.7865 - val_loss: 0.4922 - val_accuracy: 0.8156\n",
      "Epoch 200/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.7823 - val_loss: 0.4860 - val_accuracy: 0.8380\n",
      "Epoch 201/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5506 - accuracy: 0.7879 - val_loss: 0.4776 - val_accuracy: 0.8268\n",
      "Epoch 202/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5457 - accuracy: 0.7879 - val_loss: 0.4828 - val_accuracy: 0.8324\n",
      "Epoch 203/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5468 - accuracy: 0.7823 - val_loss: 0.4846 - val_accuracy: 0.8268\n",
      "Epoch 204/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5473 - accuracy: 0.7935 - val_loss: 0.4839 - val_accuracy: 0.8268\n",
      "Epoch 205/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5453 - accuracy: 0.7921 - val_loss: 0.4884 - val_accuracy: 0.8268\n",
      "Epoch 206/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.7837 - val_loss: 0.4793 - val_accuracy: 0.8324\n",
      "Epoch 207/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7823 - val_loss: 0.4862 - val_accuracy: 0.8268\n",
      "Epoch 208/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5453 - accuracy: 0.7865 - val_loss: 0.4806 - val_accuracy: 0.8380\n",
      "Epoch 209/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7921 - val_loss: 0.4798 - val_accuracy: 0.8380\n",
      "Epoch 210/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7893 - val_loss: 0.4783 - val_accuracy: 0.8380\n",
      "Epoch 211/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7851 - val_loss: 0.4785 - val_accuracy: 0.8324\n",
      "Epoch 212/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5422 - accuracy: 0.7837 - val_loss: 0.4909 - val_accuracy: 0.8268\n",
      "Epoch 213/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5446 - accuracy: 0.7851 - val_loss: 0.4767 - val_accuracy: 0.8380\n",
      "Epoch 214/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5422 - accuracy: 0.7865 - val_loss: 0.4755 - val_accuracy: 0.8324\n",
      "Epoch 215/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5426 - accuracy: 0.7907 - val_loss: 0.4760 - val_accuracy: 0.8324\n",
      "Epoch 216/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5425 - accuracy: 0.7865 - val_loss: 0.4800 - val_accuracy: 0.8380\n",
      "Epoch 217/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7907 - val_loss: 0.4766 - val_accuracy: 0.8324\n",
      "Epoch 218/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.7978 - val_loss: 0.4785 - val_accuracy: 0.8268\n",
      "Epoch 219/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.7837 - val_loss: 0.4764 - val_accuracy: 0.8380\n",
      "Epoch 220/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.7935 - val_loss: 0.4758 - val_accuracy: 0.8436\n",
      "Epoch 221/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5423 - accuracy: 0.7851 - val_loss: 0.4756 - val_accuracy: 0.8380\n",
      "Epoch 222/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5391 - accuracy: 0.7949 - val_loss: 0.4739 - val_accuracy: 0.8324\n",
      "Epoch 223/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5422 - accuracy: 0.7949 - val_loss: 0.4714 - val_accuracy: 0.8324\n",
      "Epoch 224/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7879 - val_loss: 0.4920 - val_accuracy: 0.8101\n",
      "Epoch 225/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.7921 - val_loss: 0.4702 - val_accuracy: 0.8324\n",
      "Epoch 226/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7851 - val_loss: 0.4762 - val_accuracy: 0.8268\n",
      "Epoch 227/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.7921 - val_loss: 0.4730 - val_accuracy: 0.8324\n",
      "Epoch 228/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7809 - val_loss: 0.4813 - val_accuracy: 0.8268\n",
      "Epoch 229/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.7893 - val_loss: 0.4722 - val_accuracy: 0.8380\n",
      "Epoch 230/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.7893 - val_loss: 0.4708 - val_accuracy: 0.8380\n",
      "Epoch 231/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.7795 - val_loss: 0.4715 - val_accuracy: 0.8324\n",
      "Epoch 232/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5398 - accuracy: 0.7893 - val_loss: 0.4694 - val_accuracy: 0.8380\n",
      "Epoch 233/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7851 - val_loss: 0.4725 - val_accuracy: 0.8268\n",
      "Epoch 234/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7921 - val_loss: 0.4702 - val_accuracy: 0.8436\n",
      "Epoch 235/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7935 - val_loss: 0.4706 - val_accuracy: 0.8380\n",
      "Epoch 236/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5384 - accuracy: 0.7879 - val_loss: 0.4695 - val_accuracy: 0.8380\n",
      "Epoch 237/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5369 - accuracy: 0.7921 - val_loss: 0.4703 - val_accuracy: 0.8380\n",
      "Epoch 238/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7978 - val_loss: 0.4708 - val_accuracy: 0.8436\n",
      "Epoch 239/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7921 - val_loss: 0.4711 - val_accuracy: 0.8268\n",
      "Epoch 240/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5400 - accuracy: 0.7949 - val_loss: 0.4729 - val_accuracy: 0.8324\n",
      "Epoch 241/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.7907 - val_loss: 0.4750 - val_accuracy: 0.8324\n",
      "Epoch 242/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7851 - val_loss: 0.4690 - val_accuracy: 0.8380\n",
      "Epoch 243/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.7837 - val_loss: 0.4702 - val_accuracy: 0.8380\n",
      "Epoch 244/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7949 - val_loss: 0.4688 - val_accuracy: 0.8380\n",
      "Epoch 245/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5358 - accuracy: 0.7823 - val_loss: 0.4712 - val_accuracy: 0.8380\n",
      "Epoch 246/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7992 - val_loss: 0.4653 - val_accuracy: 0.8380\n",
      "Epoch 247/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7851 - val_loss: 0.4687 - val_accuracy: 0.8324\n",
      "Epoch 248/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.7963 - val_loss: 0.4709 - val_accuracy: 0.8324\n",
      "Epoch 249/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.7907 - val_loss: 0.4654 - val_accuracy: 0.8324\n",
      "Epoch 250/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5347 - accuracy: 0.7879 - val_loss: 0.4736 - val_accuracy: 0.8268\n",
      "Epoch 251/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7865 - val_loss: 0.4729 - val_accuracy: 0.8268\n",
      "Epoch 252/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5351 - accuracy: 0.7879 - val_loss: 0.4673 - val_accuracy: 0.8380\n",
      "Epoch 253/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7935 - val_loss: 0.4739 - val_accuracy: 0.8324\n",
      "Epoch 254/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5405 - accuracy: 0.7851 - val_loss: 0.4685 - val_accuracy: 0.8324\n",
      "Epoch 255/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7992 - val_loss: 0.4707 - val_accuracy: 0.8268\n",
      "Epoch 256/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7907 - val_loss: 0.4645 - val_accuracy: 0.8324\n",
      "Epoch 257/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7893 - val_loss: 0.4696 - val_accuracy: 0.8324\n",
      "Epoch 258/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5323 - accuracy: 0.7879 - val_loss: 0.4751 - val_accuracy: 0.8268\n",
      "Epoch 259/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.5360 - accuracy: 0.7851 - val_loss: 0.4627 - val_accuracy: 0.8380\n",
      "Epoch 260/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.5316 - accuracy: 0.8006 - val_loss: 0.4722 - val_accuracy: 0.8268\n",
      "Epoch 261/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5337 - accuracy: 0.7851 - val_loss: 0.4625 - val_accuracy: 0.8380\n",
      "Epoch 262/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7935 - val_loss: 0.4696 - val_accuracy: 0.8268\n",
      "Epoch 263/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5321 - accuracy: 0.7851 - val_loss: 0.4627 - val_accuracy: 0.8380\n",
      "Epoch 264/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.7893 - val_loss: 0.4776 - val_accuracy: 0.8268\n",
      "Epoch 265/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.7907 - val_loss: 0.4818 - val_accuracy: 0.8212\n",
      "Epoch 266/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.7935 - val_loss: 0.4655 - val_accuracy: 0.8324\n",
      "Epoch 267/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5321 - accuracy: 0.7921 - val_loss: 0.4723 - val_accuracy: 0.8268\n",
      "Epoch 268/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5320 - accuracy: 0.7837 - val_loss: 0.4623 - val_accuracy: 0.8380\n",
      "Epoch 269/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7978 - val_loss: 0.4683 - val_accuracy: 0.8324\n",
      "Epoch 270/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7907 - val_loss: 0.4628 - val_accuracy: 0.8436\n",
      "Epoch 271/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7935 - val_loss: 0.4627 - val_accuracy: 0.8380\n",
      "Epoch 272/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7893 - val_loss: 0.4654 - val_accuracy: 0.8268\n",
      "Epoch 273/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.7935 - val_loss: 0.4627 - val_accuracy: 0.8436\n",
      "Epoch 274/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5301 - accuracy: 0.7893 - val_loss: 0.4640 - val_accuracy: 0.8268\n",
      "Epoch 275/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.7907 - val_loss: 0.4625 - val_accuracy: 0.8268\n",
      "Epoch 276/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7992 - val_loss: 0.4661 - val_accuracy: 0.8268\n",
      "Epoch 277/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7893 - val_loss: 0.4695 - val_accuracy: 0.8268\n",
      "Epoch 278/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7809 - val_loss: 0.4613 - val_accuracy: 0.8436\n",
      "Epoch 279/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7963 - val_loss: 0.4688 - val_accuracy: 0.8268\n",
      "Epoch 280/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7823 - val_loss: 0.4579 - val_accuracy: 0.8380\n",
      "Epoch 281/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5283 - accuracy: 0.7851 - val_loss: 0.4604 - val_accuracy: 0.8380\n",
      "Epoch 282/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.7879 - val_loss: 0.4621 - val_accuracy: 0.8268\n",
      "Epoch 283/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5263 - accuracy: 0.7837 - val_loss: 0.4611 - val_accuracy: 0.8268\n",
      "Epoch 284/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5272 - accuracy: 0.7963 - val_loss: 0.4584 - val_accuracy: 0.8380\n",
      "Epoch 285/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7879 - val_loss: 0.4614 - val_accuracy: 0.8380\n",
      "Epoch 286/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5258 - accuracy: 0.7865 - val_loss: 0.4588 - val_accuracy: 0.8324\n",
      "Epoch 287/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7851 - val_loss: 0.4625 - val_accuracy: 0.8380\n",
      "Epoch 288/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5247 - accuracy: 0.7935 - val_loss: 0.4652 - val_accuracy: 0.8324\n",
      "Epoch 289/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5263 - accuracy: 0.7921 - val_loss: 0.4600 - val_accuracy: 0.8380\n",
      "Epoch 290/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.7949 - val_loss: 0.4622 - val_accuracy: 0.8380\n",
      "Epoch 291/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7921 - val_loss: 0.4562 - val_accuracy: 0.8380\n",
      "Epoch 292/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7907 - val_loss: 0.4596 - val_accuracy: 0.8380\n",
      "Epoch 293/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7949 - val_loss: 0.4548 - val_accuracy: 0.8380\n",
      "Epoch 294/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5236 - accuracy: 0.7865 - val_loss: 0.4590 - val_accuracy: 0.8380\n",
      "Epoch 295/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7851 - val_loss: 0.4574 - val_accuracy: 0.8380\n",
      "Epoch 296/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7921 - val_loss: 0.4551 - val_accuracy: 0.8380\n",
      "Epoch 297/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7865 - val_loss: 0.4629 - val_accuracy: 0.8324\n",
      "Epoch 298/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5211 - accuracy: 0.7935 - val_loss: 0.4740 - val_accuracy: 0.8268\n",
      "Epoch 299/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5264 - accuracy: 0.7851 - val_loss: 0.4586 - val_accuracy: 0.8380\n",
      "Epoch 300/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5235 - accuracy: 0.7963 - val_loss: 0.4581 - val_accuracy: 0.8380\n",
      "Epoch 301/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5229 - accuracy: 0.7879 - val_loss: 0.4580 - val_accuracy: 0.8380\n",
      "Epoch 302/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.7935 - val_loss: 0.4547 - val_accuracy: 0.8436\n",
      "Epoch 303/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.7809 - val_loss: 0.4637 - val_accuracy: 0.8380\n",
      "Epoch 304/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5231 - accuracy: 0.8048 - val_loss: 0.4613 - val_accuracy: 0.8380\n",
      "Epoch 305/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.7949 - val_loss: 0.4584 - val_accuracy: 0.8380\n",
      "Epoch 306/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7879 - val_loss: 0.4606 - val_accuracy: 0.8324\n",
      "Epoch 307/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5240 - accuracy: 0.7879 - val_loss: 0.4588 - val_accuracy: 0.8324\n",
      "Epoch 308/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5226 - accuracy: 0.7992 - val_loss: 0.4547 - val_accuracy: 0.8324\n",
      "Epoch 309/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5209 - accuracy: 0.7921 - val_loss: 0.4619 - val_accuracy: 0.8324\n",
      "Epoch 310/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5222 - accuracy: 0.7978 - val_loss: 0.4525 - val_accuracy: 0.8380\n",
      "Epoch 311/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7963 - val_loss: 0.4554 - val_accuracy: 0.8380\n",
      "Epoch 312/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7992 - val_loss: 0.4517 - val_accuracy: 0.8380\n",
      "Epoch 313/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7879 - val_loss: 0.4543 - val_accuracy: 0.8380\n",
      "Epoch 314/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.7865 - val_loss: 0.4530 - val_accuracy: 0.8324\n",
      "Epoch 315/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5202 - accuracy: 0.7949 - val_loss: 0.4566 - val_accuracy: 0.8436\n",
      "Epoch 316/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5198 - accuracy: 0.7963 - val_loss: 0.4537 - val_accuracy: 0.8324\n",
      "Epoch 317/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5208 - accuracy: 0.7837 - val_loss: 0.4543 - val_accuracy: 0.8268\n",
      "Epoch 318/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.7935 - val_loss: 0.4631 - val_accuracy: 0.8212\n",
      "Epoch 319/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7837 - val_loss: 0.4518 - val_accuracy: 0.8324\n",
      "Epoch 320/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7992 - val_loss: 0.4552 - val_accuracy: 0.8380\n",
      "Epoch 321/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5209 - accuracy: 0.7963 - val_loss: 0.4520 - val_accuracy: 0.8380\n",
      "Epoch 322/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7879 - val_loss: 0.4514 - val_accuracy: 0.8436\n",
      "Epoch 323/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.8062 - val_loss: 0.4521 - val_accuracy: 0.8380\n",
      "Epoch 324/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.8020 - val_loss: 0.4537 - val_accuracy: 0.8380\n",
      "Epoch 325/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7992 - val_loss: 0.4525 - val_accuracy: 0.8436\n",
      "Epoch 326/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7879 - val_loss: 0.4534 - val_accuracy: 0.8324\n",
      "Epoch 327/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5190 - accuracy: 0.8020 - val_loss: 0.4542 - val_accuracy: 0.8436\n",
      "Epoch 328/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.7963 - val_loss: 0.4503 - val_accuracy: 0.8324\n",
      "Epoch 329/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5190 - accuracy: 0.7865 - val_loss: 0.4517 - val_accuracy: 0.8380\n",
      "Epoch 330/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.8006 - val_loss: 0.4529 - val_accuracy: 0.8380\n",
      "Epoch 331/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5221 - accuracy: 0.7879 - val_loss: 0.4519 - val_accuracy: 0.8436\n",
      "Epoch 332/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5183 - accuracy: 0.7865 - val_loss: 0.4505 - val_accuracy: 0.8436\n",
      "Epoch 333/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.7935 - val_loss: 0.4518 - val_accuracy: 0.8436\n",
      "Epoch 334/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5165 - accuracy: 0.7893 - val_loss: 0.4484 - val_accuracy: 0.8380\n",
      "Epoch 335/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5171 - accuracy: 0.7879 - val_loss: 0.4499 - val_accuracy: 0.8324\n",
      "Epoch 336/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7949 - val_loss: 0.4514 - val_accuracy: 0.8380\n",
      "Epoch 337/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5154 - accuracy: 0.7865 - val_loss: 0.4546 - val_accuracy: 0.8268\n",
      "Epoch 338/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5174 - accuracy: 0.7949 - val_loss: 0.4478 - val_accuracy: 0.8380\n",
      "Epoch 339/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.7921 - val_loss: 0.4493 - val_accuracy: 0.8324\n",
      "Epoch 340/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.8020 - val_loss: 0.4520 - val_accuracy: 0.8380\n",
      "Epoch 341/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5176 - accuracy: 0.7907 - val_loss: 0.4587 - val_accuracy: 0.8380\n",
      "Epoch 342/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5195 - accuracy: 0.7893 - val_loss: 0.4495 - val_accuracy: 0.8436\n",
      "Epoch 343/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.8006 - val_loss: 0.4498 - val_accuracy: 0.8380\n",
      "Epoch 344/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5157 - accuracy: 0.7837 - val_loss: 0.4497 - val_accuracy: 0.8268\n",
      "Epoch 345/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7963 - val_loss: 0.4498 - val_accuracy: 0.8436\n",
      "Epoch 346/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5177 - accuracy: 0.7921 - val_loss: 0.4470 - val_accuracy: 0.8324\n",
      "Epoch 347/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.7879 - val_loss: 0.4450 - val_accuracy: 0.8380\n",
      "Epoch 348/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7851 - val_loss: 0.4477 - val_accuracy: 0.8324\n",
      "Epoch 349/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7921 - val_loss: 0.4490 - val_accuracy: 0.8268\n",
      "Epoch 350/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7978 - val_loss: 0.4483 - val_accuracy: 0.8380\n",
      "Epoch 351/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7978 - val_loss: 0.4494 - val_accuracy: 0.8324\n",
      "Epoch 352/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7978 - val_loss: 0.4502 - val_accuracy: 0.8268\n",
      "Epoch 353/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.8062 - val_loss: 0.4498 - val_accuracy: 0.8324\n",
      "Epoch 354/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7949 - val_loss: 0.4514 - val_accuracy: 0.8324\n",
      "Epoch 355/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.7935 - val_loss: 0.4459 - val_accuracy: 0.8324\n",
      "Epoch 356/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7949 - val_loss: 0.4518 - val_accuracy: 0.8212\n",
      "Epoch 357/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7893 - val_loss: 0.4503 - val_accuracy: 0.8380\n",
      "Epoch 358/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7963 - val_loss: 0.4446 - val_accuracy: 0.8324\n",
      "Epoch 359/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7949 - val_loss: 0.4480 - val_accuracy: 0.8324\n",
      "Epoch 360/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7949 - val_loss: 0.4540 - val_accuracy: 0.8324\n",
      "Epoch 361/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.8034 - val_loss: 0.4455 - val_accuracy: 0.8380\n",
      "Epoch 362/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5132 - accuracy: 0.7851 - val_loss: 0.4474 - val_accuracy: 0.8380\n",
      "Epoch 363/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7893 - val_loss: 0.4484 - val_accuracy: 0.8380\n",
      "Epoch 364/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5128 - accuracy: 0.7921 - val_loss: 0.4427 - val_accuracy: 0.8380\n",
      "Epoch 365/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7949 - val_loss: 0.4461 - val_accuracy: 0.8268\n",
      "Epoch 366/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7865 - val_loss: 0.4480 - val_accuracy: 0.8380\n",
      "Epoch 367/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7921 - val_loss: 0.4433 - val_accuracy: 0.8436\n",
      "Epoch 368/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7963 - val_loss: 0.4536 - val_accuracy: 0.8380\n",
      "Epoch 369/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7992 - val_loss: 0.4501 - val_accuracy: 0.8324\n",
      "Epoch 370/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5155 - accuracy: 0.8048 - val_loss: 0.4455 - val_accuracy: 0.8324\n",
      "Epoch 371/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5121 - accuracy: 0.7949 - val_loss: 0.4464 - val_accuracy: 0.8380\n",
      "Epoch 372/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7992 - val_loss: 0.4460 - val_accuracy: 0.8380\n",
      "Epoch 373/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7992 - val_loss: 0.4469 - val_accuracy: 0.8380\n",
      "Epoch 374/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.8006 - val_loss: 0.4556 - val_accuracy: 0.8324\n",
      "Epoch 375/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7907 - val_loss: 0.4440 - val_accuracy: 0.8380\n",
      "Epoch 376/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7935 - val_loss: 0.4452 - val_accuracy: 0.8380\n",
      "Epoch 377/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5113 - accuracy: 0.8020 - val_loss: 0.4409 - val_accuracy: 0.8436\n",
      "Epoch 378/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7949 - val_loss: 0.4438 - val_accuracy: 0.8436\n",
      "Epoch 379/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7907 - val_loss: 0.4447 - val_accuracy: 0.8380\n",
      "Epoch 380/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5112 - accuracy: 0.7921 - val_loss: 0.4421 - val_accuracy: 0.8324\n",
      "Epoch 381/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7992 - val_loss: 0.4425 - val_accuracy: 0.8380\n",
      "Epoch 382/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.8006 - val_loss: 0.4397 - val_accuracy: 0.8436\n",
      "Epoch 383/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5104 - accuracy: 0.8006 - val_loss: 0.4409 - val_accuracy: 0.8380\n",
      "Epoch 384/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5107 - accuracy: 0.7907 - val_loss: 0.4411 - val_accuracy: 0.8324\n",
      "Epoch 385/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.8020 - val_loss: 0.4474 - val_accuracy: 0.8380\n",
      "Epoch 386/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7949 - val_loss: 0.4430 - val_accuracy: 0.8324\n",
      "Epoch 387/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5127 - accuracy: 0.7978 - val_loss: 0.4401 - val_accuracy: 0.8324\n",
      "Epoch 388/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5094 - accuracy: 0.7963 - val_loss: 0.4412 - val_accuracy: 0.8324\n",
      "Epoch 389/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.7893 - val_loss: 0.4410 - val_accuracy: 0.8380\n",
      "Epoch 390/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7978 - val_loss: 0.4487 - val_accuracy: 0.8324\n",
      "Epoch 391/10000\n",
      "178/178 [==============================] - 0s 1ms/step - loss: 0.5097 - accuracy: 0.7921 - val_loss: 0.4413 - val_accuracy: 0.8436\n",
      "Epoch 392/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7949 - val_loss: 0.4375 - val_accuracy: 0.8380\n",
      "Epoch 393/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7935 - val_loss: 0.4423 - val_accuracy: 0.8324\n",
      "Epoch 394/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7921 - val_loss: 0.4423 - val_accuracy: 0.8380\n",
      "Epoch 395/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7949 - val_loss: 0.4424 - val_accuracy: 0.8268\n",
      "Epoch 396/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7893 - val_loss: 0.4434 - val_accuracy: 0.8324\n",
      "Epoch 397/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7963 - val_loss: 0.4488 - val_accuracy: 0.8380\n",
      "Epoch 398/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.8048 - val_loss: 0.4452 - val_accuracy: 0.8324\n",
      "Epoch 399/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7963 - val_loss: 0.4403 - val_accuracy: 0.8436\n",
      "Epoch 400/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.8034 - val_loss: 0.4510 - val_accuracy: 0.8324\n",
      "Epoch 401/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7978 - val_loss: 0.4515 - val_accuracy: 0.8212\n",
      "Epoch 402/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7992 - val_loss: 0.4397 - val_accuracy: 0.8324\n",
      "Epoch 403/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7992 - val_loss: 0.4625 - val_accuracy: 0.8156\n",
      "Epoch 404/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7949 - val_loss: 0.4423 - val_accuracy: 0.8268\n",
      "Epoch 405/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7935 - val_loss: 0.4393 - val_accuracy: 0.8324\n",
      "Epoch 406/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.7935 - val_loss: 0.4407 - val_accuracy: 0.8380\n",
      "Epoch 407/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.8034 - val_loss: 0.4381 - val_accuracy: 0.8324\n",
      "Epoch 408/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7992 - val_loss: 0.4377 - val_accuracy: 0.8380\n",
      "Epoch 409/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5070 - accuracy: 0.7992 - val_loss: 0.4406 - val_accuracy: 0.8380\n",
      "Epoch 410/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7978 - val_loss: 0.4439 - val_accuracy: 0.8324\n",
      "Epoch 411/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5081 - accuracy: 0.7935 - val_loss: 0.4394 - val_accuracy: 0.8380\n",
      "Epoch 412/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7963 - val_loss: 0.4445 - val_accuracy: 0.8268\n",
      "Epoch 413/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5080 - accuracy: 0.7893 - val_loss: 0.4409 - val_accuracy: 0.8324\n",
      "Epoch 414/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7963 - val_loss: 0.4390 - val_accuracy: 0.8380\n",
      "Epoch 415/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5052 - accuracy: 0.7992 - val_loss: 0.4375 - val_accuracy: 0.8380\n",
      "Epoch 416/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.7963 - val_loss: 0.4361 - val_accuracy: 0.8380\n",
      "Epoch 417/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7978 - val_loss: 0.4357 - val_accuracy: 0.8380\n",
      "Epoch 418/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.8006 - val_loss: 0.4449 - val_accuracy: 0.8268\n",
      "Epoch 419/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7921 - val_loss: 0.4389 - val_accuracy: 0.8380\n",
      "Epoch 420/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7992 - val_loss: 0.4511 - val_accuracy: 0.8268\n",
      "Epoch 421/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7978 - val_loss: 0.4354 - val_accuracy: 0.8380\n",
      "Epoch 422/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7963 - val_loss: 0.4381 - val_accuracy: 0.8324\n",
      "Epoch 423/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7963 - val_loss: 0.4368 - val_accuracy: 0.8324\n",
      "Epoch 424/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7949 - val_loss: 0.4342 - val_accuracy: 0.8324\n",
      "Epoch 425/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.8006 - val_loss: 0.4353 - val_accuracy: 0.8380\n",
      "Epoch 426/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5053 - accuracy: 0.7949 - val_loss: 0.4434 - val_accuracy: 0.8212\n",
      "Epoch 427/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.7978 - val_loss: 0.4354 - val_accuracy: 0.8324\n",
      "Epoch 428/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5053 - accuracy: 0.7963 - val_loss: 0.4338 - val_accuracy: 0.8380\n",
      "Epoch 429/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7949 - val_loss: 0.4345 - val_accuracy: 0.8324\n",
      "Epoch 430/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7921 - val_loss: 0.4395 - val_accuracy: 0.8380\n",
      "Epoch 431/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5045 - accuracy: 0.7992 - val_loss: 0.4426 - val_accuracy: 0.8212\n",
      "Epoch 432/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7992 - val_loss: 0.4366 - val_accuracy: 0.8380\n",
      "Epoch 433/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7963 - val_loss: 0.4361 - val_accuracy: 0.8380\n",
      "Epoch 434/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.7992 - val_loss: 0.4390 - val_accuracy: 0.8212\n",
      "Epoch 435/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7978 - val_loss: 0.4352 - val_accuracy: 0.8380\n",
      "Epoch 436/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7978 - val_loss: 0.4395 - val_accuracy: 0.8268\n",
      "Epoch 437/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7949 - val_loss: 0.4372 - val_accuracy: 0.8324\n",
      "Epoch 438/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.8076 - val_loss: 0.4513 - val_accuracy: 0.8101\n",
      "Epoch 439/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.8020 - val_loss: 0.4433 - val_accuracy: 0.8212\n",
      "Epoch 440/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7949 - val_loss: 0.4370 - val_accuracy: 0.8380\n",
      "Epoch 441/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7949 - val_loss: 0.4368 - val_accuracy: 0.8268\n",
      "Epoch 442/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7949 - val_loss: 0.4343 - val_accuracy: 0.8324\n",
      "Epoch 443/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7963 - val_loss: 0.4438 - val_accuracy: 0.8324\n",
      "Epoch 444/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7978 - val_loss: 0.4361 - val_accuracy: 0.8380\n",
      "Epoch 445/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5016 - accuracy: 0.7963 - val_loss: 0.4328 - val_accuracy: 0.8380\n",
      "Epoch 446/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.8006 - val_loss: 0.4439 - val_accuracy: 0.8324\n",
      "Epoch 447/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7978 - val_loss: 0.4352 - val_accuracy: 0.8324\n",
      "Epoch 448/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4999 - accuracy: 0.8048 - val_loss: 0.4352 - val_accuracy: 0.8380\n",
      "Epoch 449/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.8020 - val_loss: 0.4359 - val_accuracy: 0.8324\n",
      "Epoch 450/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.8006 - val_loss: 0.4331 - val_accuracy: 0.8380\n",
      "Epoch 451/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7992 - val_loss: 0.4333 - val_accuracy: 0.8324\n",
      "Epoch 452/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7992 - val_loss: 0.4325 - val_accuracy: 0.8380\n",
      "Epoch 453/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.8020 - val_loss: 0.4343 - val_accuracy: 0.8324\n",
      "Epoch 454/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.8020 - val_loss: 0.4360 - val_accuracy: 0.8324\n",
      "Epoch 455/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7949 - val_loss: 0.4346 - val_accuracy: 0.8324\n",
      "Epoch 456/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.8076 - val_loss: 0.4367 - val_accuracy: 0.8268\n",
      "Epoch 457/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.8034 - val_loss: 0.4366 - val_accuracy: 0.8212\n",
      "Epoch 458/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.7935 - val_loss: 0.4338 - val_accuracy: 0.8324\n",
      "Epoch 459/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.8020 - val_loss: 0.4354 - val_accuracy: 0.8380\n",
      "Epoch 460/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.8034 - val_loss: 0.4346 - val_accuracy: 0.8380\n",
      "Epoch 461/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7978 - val_loss: 0.4344 - val_accuracy: 0.8380\n",
      "Epoch 462/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7992 - val_loss: 0.4341 - val_accuracy: 0.8380\n",
      "Epoch 463/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.8020 - val_loss: 0.4336 - val_accuracy: 0.8324\n",
      "Epoch 464/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5013 - accuracy: 0.7992 - val_loss: 0.4311 - val_accuracy: 0.8380\n",
      "Epoch 465/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7992 - val_loss: 0.4478 - val_accuracy: 0.8212\n",
      "Epoch 466/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7935 - val_loss: 0.4321 - val_accuracy: 0.8324\n",
      "Epoch 467/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7935 - val_loss: 0.4312 - val_accuracy: 0.8380\n",
      "Epoch 468/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7978 - val_loss: 0.4339 - val_accuracy: 0.8380\n",
      "Epoch 469/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.7963 - val_loss: 0.4342 - val_accuracy: 0.8268\n",
      "Epoch 470/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.8020 - val_loss: 0.4325 - val_accuracy: 0.8268\n",
      "Epoch 471/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7963 - val_loss: 0.4299 - val_accuracy: 0.8380\n",
      "Epoch 472/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.8020 - val_loss: 0.4316 - val_accuracy: 0.8324\n",
      "Epoch 473/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.8048 - val_loss: 0.4361 - val_accuracy: 0.8212\n",
      "Epoch 474/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7963 - val_loss: 0.4336 - val_accuracy: 0.8380\n",
      "Epoch 475/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.7992 - val_loss: 0.4299 - val_accuracy: 0.8380\n",
      "Epoch 476/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.7992 - val_loss: 0.4335 - val_accuracy: 0.8324\n",
      "Epoch 477/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.8006 - val_loss: 0.4376 - val_accuracy: 0.8268\n",
      "Epoch 478/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.7992 - val_loss: 0.4390 - val_accuracy: 0.8324\n",
      "Epoch 479/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7978 - val_loss: 0.4322 - val_accuracy: 0.8324\n",
      "Epoch 480/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4970 - accuracy: 0.7978 - val_loss: 0.4300 - val_accuracy: 0.8324\n",
      "Epoch 481/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.8020 - val_loss: 0.4306 - val_accuracy: 0.8324\n",
      "Epoch 482/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7978 - val_loss: 0.4342 - val_accuracy: 0.8380\n",
      "Epoch 483/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.8006 - val_loss: 0.4327 - val_accuracy: 0.8324\n",
      "Epoch 484/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7992 - val_loss: 0.4349 - val_accuracy: 0.8324\n",
      "Epoch 485/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7978 - val_loss: 0.4318 - val_accuracy: 0.8324\n",
      "Epoch 486/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.7978 - val_loss: 0.4304 - val_accuracy: 0.8324\n",
      "Epoch 487/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.8062 - val_loss: 0.4295 - val_accuracy: 0.8324\n",
      "Epoch 488/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7949 - val_loss: 0.4400 - val_accuracy: 0.8212\n",
      "Epoch 489/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.8020 - val_loss: 0.4310 - val_accuracy: 0.8380\n",
      "Epoch 490/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.8006 - val_loss: 0.4300 - val_accuracy: 0.8324\n",
      "Epoch 491/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.8006 - val_loss: 0.4386 - val_accuracy: 0.8156\n",
      "Epoch 492/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7992 - val_loss: 0.4293 - val_accuracy: 0.8324\n",
      "Epoch 493/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.8006 - val_loss: 0.4388 - val_accuracy: 0.8212\n",
      "Epoch 494/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7935 - val_loss: 0.4286 - val_accuracy: 0.8324\n",
      "Epoch 495/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7949 - val_loss: 0.4334 - val_accuracy: 0.8212\n",
      "Epoch 496/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.8020 - val_loss: 0.4280 - val_accuracy: 0.8324\n",
      "Epoch 497/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.8006 - val_loss: 0.4350 - val_accuracy: 0.8156\n",
      "Epoch 498/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.8062 - val_loss: 0.4354 - val_accuracy: 0.8324\n",
      "Epoch 499/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.8020 - val_loss: 0.4253 - val_accuracy: 0.8268\n",
      "Epoch 500/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7879 - val_loss: 0.4365 - val_accuracy: 0.8156\n",
      "Epoch 501/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.7935 - val_loss: 0.4297 - val_accuracy: 0.8268\n",
      "Epoch 502/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.7949 - val_loss: 0.4367 - val_accuracy: 0.8324\n",
      "Epoch 503/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.8020 - val_loss: 0.4300 - val_accuracy: 0.8380\n",
      "Epoch 504/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.8020 - val_loss: 0.4327 - val_accuracy: 0.8156\n",
      "Epoch 505/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.7992 - val_loss: 0.4276 - val_accuracy: 0.8324\n",
      "Epoch 506/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.8020 - val_loss: 0.4301 - val_accuracy: 0.8380\n",
      "Epoch 507/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.8020 - val_loss: 0.4370 - val_accuracy: 0.8212\n",
      "Epoch 508/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7992 - val_loss: 0.4347 - val_accuracy: 0.8212\n",
      "Epoch 509/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7921 - val_loss: 0.4290 - val_accuracy: 0.8324\n",
      "Epoch 510/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.7992 - val_loss: 0.4289 - val_accuracy: 0.8324\n",
      "Epoch 511/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.8048 - val_loss: 0.4265 - val_accuracy: 0.8380\n",
      "Epoch 512/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.8048 - val_loss: 0.4250 - val_accuracy: 0.8268\n",
      "Epoch 513/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.8048 - val_loss: 0.4254 - val_accuracy: 0.8324\n",
      "Epoch 514/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7992 - val_loss: 0.4317 - val_accuracy: 0.8156\n",
      "Epoch 515/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7935 - val_loss: 0.4374 - val_accuracy: 0.8156\n",
      "Epoch 516/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.8006 - val_loss: 0.4265 - val_accuracy: 0.8380\n",
      "Epoch 517/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.8048 - val_loss: 0.4280 - val_accuracy: 0.8380\n",
      "Epoch 518/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.8006 - val_loss: 0.4293 - val_accuracy: 0.8380\n",
      "Epoch 519/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.8020 - val_loss: 0.4260 - val_accuracy: 0.8324\n",
      "Epoch 520/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.8006 - val_loss: 0.4252 - val_accuracy: 0.8324\n",
      "Epoch 521/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.8006 - val_loss: 0.4279 - val_accuracy: 0.8380\n",
      "Epoch 522/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7963 - val_loss: 0.4324 - val_accuracy: 0.8268\n",
      "Epoch 523/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.7992 - val_loss: 0.4255 - val_accuracy: 0.8324\n",
      "Epoch 524/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.8006 - val_loss: 0.4262 - val_accuracy: 0.8268\n",
      "Epoch 525/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.8006 - val_loss: 0.4262 - val_accuracy: 0.8268\n",
      "Epoch 526/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.8020 - val_loss: 0.4292 - val_accuracy: 0.8212\n",
      "Epoch 527/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.8020 - val_loss: 0.4276 - val_accuracy: 0.8380\n",
      "Epoch 528/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7992 - val_loss: 0.4272 - val_accuracy: 0.8324\n",
      "Epoch 529/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.8020 - val_loss: 0.4319 - val_accuracy: 0.8156\n",
      "Epoch 530/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7992 - val_loss: 0.4268 - val_accuracy: 0.8324\n",
      "Epoch 531/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7992 - val_loss: 0.4267 - val_accuracy: 0.8268\n",
      "Epoch 532/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.8006 - val_loss: 0.4261 - val_accuracy: 0.8380\n",
      "Epoch 533/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4917 - accuracy: 0.7963 - val_loss: 0.4336 - val_accuracy: 0.8101\n",
      "Epoch 534/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.8020 - val_loss: 0.4247 - val_accuracy: 0.8268\n",
      "Epoch 535/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.8006 - val_loss: 0.4240 - val_accuracy: 0.8324\n",
      "Epoch 536/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.8006 - val_loss: 0.4252 - val_accuracy: 0.8380\n",
      "Epoch 537/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.8020 - val_loss: 0.4316 - val_accuracy: 0.8212\n",
      "Epoch 538/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7978 - val_loss: 0.4275 - val_accuracy: 0.8156\n",
      "Epoch 539/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.7978 - val_loss: 0.4238 - val_accuracy: 0.8324\n",
      "Epoch 540/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7949 - val_loss: 0.4273 - val_accuracy: 0.8324\n",
      "Epoch 541/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7978 - val_loss: 0.4261 - val_accuracy: 0.8268\n",
      "Epoch 542/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7992 - val_loss: 0.4246 - val_accuracy: 0.8324\n",
      "Epoch 543/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.8020 - val_loss: 0.4247 - val_accuracy: 0.8268\n",
      "Epoch 544/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.8034 - val_loss: 0.4266 - val_accuracy: 0.8212\n",
      "Epoch 545/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.7992 - val_loss: 0.4312 - val_accuracy: 0.8212\n",
      "Epoch 546/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.8048 - val_loss: 0.4235 - val_accuracy: 0.8324\n",
      "Epoch 547/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4920 - accuracy: 0.7949 - val_loss: 0.4253 - val_accuracy: 0.8268\n",
      "Epoch 548/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.8048 - val_loss: 0.4220 - val_accuracy: 0.8324\n",
      "Epoch 549/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.8034 - val_loss: 0.4231 - val_accuracy: 0.8268\n",
      "Epoch 550/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.8006 - val_loss: 0.4266 - val_accuracy: 0.8101\n",
      "Epoch 551/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7963 - val_loss: 0.4260 - val_accuracy: 0.8324\n",
      "Epoch 552/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.7978 - val_loss: 0.4276 - val_accuracy: 0.8212\n",
      "Epoch 553/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.8020 - val_loss: 0.4296 - val_accuracy: 0.8212\n",
      "Epoch 554/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.7992 - val_loss: 0.4221 - val_accuracy: 0.8268\n",
      "Epoch 555/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.8034 - val_loss: 0.4215 - val_accuracy: 0.8268\n",
      "Epoch 556/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.8034 - val_loss: 0.4230 - val_accuracy: 0.8380\n",
      "Epoch 557/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8034 - val_loss: 0.4220 - val_accuracy: 0.8324\n",
      "Epoch 558/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7992 - val_loss: 0.4232 - val_accuracy: 0.8380\n",
      "Epoch 559/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7992 - val_loss: 0.4224 - val_accuracy: 0.8268\n",
      "Epoch 560/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.8006 - val_loss: 0.4399 - val_accuracy: 0.8212\n",
      "Epoch 561/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.8020 - val_loss: 0.4246 - val_accuracy: 0.8324\n",
      "Epoch 562/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.8006 - val_loss: 0.4263 - val_accuracy: 0.8380\n",
      "Epoch 563/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.8034 - val_loss: 0.4241 - val_accuracy: 0.8324\n",
      "Epoch 564/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.8020 - val_loss: 0.4203 - val_accuracy: 0.8324\n",
      "Epoch 565/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.8006 - val_loss: 0.4219 - val_accuracy: 0.8380\n",
      "Epoch 566/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7907 - val_loss: 0.4242 - val_accuracy: 0.8268\n",
      "Epoch 567/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.8048 - val_loss: 0.4222 - val_accuracy: 0.8324\n",
      "Epoch 568/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8034 - val_loss: 0.4242 - val_accuracy: 0.8268\n",
      "Epoch 569/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7949 - val_loss: 0.4262 - val_accuracy: 0.8324\n",
      "Epoch 570/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.8048 - val_loss: 0.4223 - val_accuracy: 0.8268\n",
      "Epoch 571/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.8048 - val_loss: 0.4173 - val_accuracy: 0.8268\n",
      "Epoch 572/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4879 - accuracy: 0.7992 - val_loss: 0.4219 - val_accuracy: 0.8268\n",
      "Epoch 573/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.8034 - val_loss: 0.4226 - val_accuracy: 0.8324\n",
      "Epoch 574/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.8062 - val_loss: 0.4232 - val_accuracy: 0.8324\n",
      "Epoch 575/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8020 - val_loss: 0.4287 - val_accuracy: 0.8156\n",
      "Epoch 576/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7893 - val_loss: 0.4223 - val_accuracy: 0.8268\n",
      "Epoch 577/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7992 - val_loss: 0.4236 - val_accuracy: 0.8324\n",
      "Epoch 578/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.8020 - val_loss: 0.4243 - val_accuracy: 0.8156\n",
      "Epoch 579/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.8020 - val_loss: 0.4236 - val_accuracy: 0.8212\n",
      "Epoch 580/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.7907 - val_loss: 0.4235 - val_accuracy: 0.8324\n",
      "Epoch 581/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8020 - val_loss: 0.4202 - val_accuracy: 0.8268\n",
      "Epoch 582/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.8034 - val_loss: 0.4231 - val_accuracy: 0.8268\n",
      "Epoch 583/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4885 - accuracy: 0.8020 - val_loss: 0.4225 - val_accuracy: 0.8268\n",
      "Epoch 584/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7978 - val_loss: 0.4220 - val_accuracy: 0.8268\n",
      "Epoch 585/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.8006 - val_loss: 0.4209 - val_accuracy: 0.8380\n",
      "Epoch 586/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.8020 - val_loss: 0.4217 - val_accuracy: 0.8324\n",
      "Epoch 587/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8006 - val_loss: 0.4261 - val_accuracy: 0.8212\n",
      "Epoch 588/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.8020 - val_loss: 0.4217 - val_accuracy: 0.8324\n",
      "Epoch 589/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.8020 - val_loss: 0.4228 - val_accuracy: 0.8268\n",
      "Epoch 590/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7949 - val_loss: 0.4283 - val_accuracy: 0.8212\n",
      "Epoch 591/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7992 - val_loss: 0.4236 - val_accuracy: 0.8156\n",
      "Epoch 592/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.8034 - val_loss: 0.4280 - val_accuracy: 0.8212\n",
      "Epoch 593/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4836 - accuracy: 0.7907 - val_loss: 0.4220 - val_accuracy: 0.8268\n",
      "Epoch 594/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.8034 - val_loss: 0.4197 - val_accuracy: 0.8324\n",
      "Epoch 595/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.8006 - val_loss: 0.4175 - val_accuracy: 0.8268\n",
      "Epoch 596/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.7907 - val_loss: 0.4206 - val_accuracy: 0.8268\n",
      "Epoch 597/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.7992 - val_loss: 0.4182 - val_accuracy: 0.8212\n",
      "Epoch 598/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.8006 - val_loss: 0.4293 - val_accuracy: 0.8156\n",
      "Epoch 599/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.8006 - val_loss: 0.4224 - val_accuracy: 0.8268\n",
      "Epoch 600/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.8048 - val_loss: 0.4227 - val_accuracy: 0.8268\n",
      "Epoch 601/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.8006 - val_loss: 0.4191 - val_accuracy: 0.8268\n",
      "Epoch 602/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.8020 - val_loss: 0.4243 - val_accuracy: 0.8156\n",
      "Epoch 603/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.8020 - val_loss: 0.4210 - val_accuracy: 0.8212\n",
      "Epoch 604/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.8006 - val_loss: 0.4277 - val_accuracy: 0.8156\n",
      "Epoch 605/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.8020 - val_loss: 0.4199 - val_accuracy: 0.8268\n",
      "Epoch 606/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.8006 - val_loss: 0.4190 - val_accuracy: 0.8268\n",
      "Epoch 607/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7992 - val_loss: 0.4186 - val_accuracy: 0.8212\n",
      "Epoch 608/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.8006 - val_loss: 0.4192 - val_accuracy: 0.8268\n",
      "Epoch 609/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7963 - val_loss: 0.4234 - val_accuracy: 0.8212\n",
      "Epoch 610/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7992 - val_loss: 0.4189 - val_accuracy: 0.8268\n",
      "Epoch 611/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8034 - val_loss: 0.4231 - val_accuracy: 0.8156\n",
      "Epoch 612/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.8006 - val_loss: 0.4204 - val_accuracy: 0.8324\n",
      "Epoch 613/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.8006 - val_loss: 0.4152 - val_accuracy: 0.8268\n",
      "Epoch 614/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7963 - val_loss: 0.4177 - val_accuracy: 0.8268\n",
      "Epoch 615/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.7992 - val_loss: 0.4190 - val_accuracy: 0.8156\n",
      "Epoch 616/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7992 - val_loss: 0.4235 - val_accuracy: 0.8101\n",
      "Epoch 617/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.8006 - val_loss: 0.4165 - val_accuracy: 0.8212\n",
      "Epoch 618/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.8006 - val_loss: 0.4205 - val_accuracy: 0.8156\n",
      "Epoch 619/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.8020 - val_loss: 0.4186 - val_accuracy: 0.8268\n",
      "Epoch 620/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7963 - val_loss: 0.4235 - val_accuracy: 0.8324\n",
      "Epoch 621/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.8048 - val_loss: 0.4211 - val_accuracy: 0.8268\n",
      "Epoch 622/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.7949 - val_loss: 0.4201 - val_accuracy: 0.8324\n",
      "Epoch 623/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.8006 - val_loss: 0.4179 - val_accuracy: 0.8268\n",
      "Epoch 624/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.8006 - val_loss: 0.4208 - val_accuracy: 0.8101\n",
      "Epoch 625/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.8006 - val_loss: 0.4199 - val_accuracy: 0.8212\n",
      "Epoch 626/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.8020 - val_loss: 0.4191 - val_accuracy: 0.8156\n",
      "Epoch 627/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7992 - val_loss: 0.4193 - val_accuracy: 0.8268\n",
      "Epoch 628/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.8020 - val_loss: 0.4197 - val_accuracy: 0.8212\n",
      "Epoch 629/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.8034 - val_loss: 0.4195 - val_accuracy: 0.8156\n",
      "Epoch 630/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.8034 - val_loss: 0.4199 - val_accuracy: 0.8156\n",
      "Epoch 631/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.8020 - val_loss: 0.4167 - val_accuracy: 0.8212\n",
      "Epoch 632/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4835 - accuracy: 0.7992 - val_loss: 0.4214 - val_accuracy: 0.8268\n",
      "Epoch 633/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.7978 - val_loss: 0.4205 - val_accuracy: 0.8268\n",
      "Epoch 634/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8020 - val_loss: 0.4159 - val_accuracy: 0.8156\n",
      "Epoch 635/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.8006 - val_loss: 0.4174 - val_accuracy: 0.8268\n",
      "Epoch 636/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.8020 - val_loss: 0.4163 - val_accuracy: 0.8268\n",
      "Epoch 637/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.8020 - val_loss: 0.4170 - val_accuracy: 0.8268\n",
      "Epoch 638/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8006 - val_loss: 0.4253 - val_accuracy: 0.8156\n",
      "Epoch 639/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.8020 - val_loss: 0.4202 - val_accuracy: 0.8268\n",
      "Epoch 640/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.8006 - val_loss: 0.4179 - val_accuracy: 0.8268\n",
      "Epoch 641/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.8006 - val_loss: 0.4204 - val_accuracy: 0.8268\n",
      "Epoch 642/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4845 - accuracy: 0.7935 - val_loss: 0.4188 - val_accuracy: 0.8268\n",
      "Epoch 643/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.8020 - val_loss: 0.4201 - val_accuracy: 0.8101\n",
      "Epoch 644/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.8006 - val_loss: 0.4215 - val_accuracy: 0.8101\n",
      "Epoch 645/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.8020 - val_loss: 0.4215 - val_accuracy: 0.8156\n",
      "Epoch 646/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7992 - val_loss: 0.4216 - val_accuracy: 0.8101\n",
      "Epoch 647/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.8034 - val_loss: 0.4195 - val_accuracy: 0.8156\n",
      "Epoch 648/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.8048 - val_loss: 0.4176 - val_accuracy: 0.8212\n",
      "Epoch 649/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.8048 - val_loss: 0.4151 - val_accuracy: 0.8156\n",
      "Epoch 650/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7963 - val_loss: 0.4161 - val_accuracy: 0.8156\n",
      "Epoch 651/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.8048 - val_loss: 0.4197 - val_accuracy: 0.8212\n",
      "Epoch 652/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8020 - val_loss: 0.4176 - val_accuracy: 0.8156\n",
      "Epoch 653/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.8020 - val_loss: 0.4198 - val_accuracy: 0.8212\n",
      "Epoch 654/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8020 - val_loss: 0.4154 - val_accuracy: 0.8268\n",
      "Epoch 655/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7992 - val_loss: 0.4170 - val_accuracy: 0.8268\n",
      "Epoch 656/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.8020 - val_loss: 0.4215 - val_accuracy: 0.8101\n",
      "Epoch 657/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7992 - val_loss: 0.4173 - val_accuracy: 0.8156\n",
      "Epoch 658/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.8006 - val_loss: 0.4180 - val_accuracy: 0.8212\n",
      "Epoch 659/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.8020 - val_loss: 0.4237 - val_accuracy: 0.8212\n",
      "Epoch 660/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7992 - val_loss: 0.4159 - val_accuracy: 0.8212\n",
      "Epoch 661/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.7949 - val_loss: 0.4157 - val_accuracy: 0.8268\n",
      "Epoch 662/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.8034 - val_loss: 0.4235 - val_accuracy: 0.8101\n",
      "Epoch 663/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4813 - accuracy: 0.8034 - val_loss: 0.4184 - val_accuracy: 0.8156\n",
      "Epoch 664/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4821 - accuracy: 0.7992 - val_loss: 0.4206 - val_accuracy: 0.8156\n",
      "Epoch 665/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.8006 - val_loss: 0.4137 - val_accuracy: 0.8156\n",
      "Epoch 666/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.8020 - val_loss: 0.4143 - val_accuracy: 0.8268\n",
      "Epoch 667/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.8020 - val_loss: 0.4155 - val_accuracy: 0.8156\n",
      "Epoch 668/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.7963 - val_loss: 0.4135 - val_accuracy: 0.8156\n",
      "Epoch 669/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.8020 - val_loss: 0.4191 - val_accuracy: 0.8212\n",
      "Epoch 670/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4790 - accuracy: 0.7978 - val_loss: 0.4209 - val_accuracy: 0.8101\n",
      "Epoch 671/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4788 - accuracy: 0.8006 - val_loss: 0.4251 - val_accuracy: 0.8101\n",
      "Epoch 672/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.8006 - val_loss: 0.4150 - val_accuracy: 0.8212\n",
      "Epoch 673/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.8006 - val_loss: 0.4138 - val_accuracy: 0.8101\n",
      "Epoch 674/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4821 - accuracy: 0.7978 - val_loss: 0.4135 - val_accuracy: 0.8156\n",
      "Epoch 675/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.7992 - val_loss: 0.4210 - val_accuracy: 0.8101\n",
      "Epoch 676/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.8006 - val_loss: 0.4186 - val_accuracy: 0.8156\n",
      "Epoch 677/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7978 - val_loss: 0.4174 - val_accuracy: 0.8212\n",
      "Epoch 678/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4775 - accuracy: 0.8034 - val_loss: 0.4237 - val_accuracy: 0.8101\n",
      "Epoch 679/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7992 - val_loss: 0.4129 - val_accuracy: 0.8212\n",
      "Epoch 680/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8020 - val_loss: 0.4159 - val_accuracy: 0.8268\n",
      "Epoch 681/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7935 - val_loss: 0.4128 - val_accuracy: 0.8212\n",
      "Epoch 682/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.8006 - val_loss: 0.4151 - val_accuracy: 0.8268\n",
      "Epoch 683/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4763 - accuracy: 0.7949 - val_loss: 0.4237 - val_accuracy: 0.8101\n",
      "Epoch 684/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.7992 - val_loss: 0.4150 - val_accuracy: 0.8156\n",
      "Epoch 685/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.8006 - val_loss: 0.4126 - val_accuracy: 0.8212\n",
      "Epoch 686/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.8020 - val_loss: 0.4167 - val_accuracy: 0.8212\n",
      "Epoch 687/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7992 - val_loss: 0.4180 - val_accuracy: 0.8268\n",
      "Epoch 688/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.8006 - val_loss: 0.4219 - val_accuracy: 0.8101\n",
      "Epoch 689/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.8020 - val_loss: 0.4150 - val_accuracy: 0.8156\n",
      "Epoch 690/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7992 - val_loss: 0.4218 - val_accuracy: 0.8212\n",
      "Epoch 691/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.8034 - val_loss: 0.4158 - val_accuracy: 0.8156\n",
      "Epoch 692/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7978 - val_loss: 0.4228 - val_accuracy: 0.8101\n",
      "Epoch 693/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.8020 - val_loss: 0.4265 - val_accuracy: 0.8101\n",
      "Epoch 694/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7921 - val_loss: 0.4160 - val_accuracy: 0.8156\n",
      "Epoch 695/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7935 - val_loss: 0.4132 - val_accuracy: 0.8212\n",
      "Epoch 696/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.8034 - val_loss: 0.4161 - val_accuracy: 0.8156\n",
      "Epoch 697/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7963 - val_loss: 0.4190 - val_accuracy: 0.8156\n",
      "Epoch 698/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7963 - val_loss: 0.4249 - val_accuracy: 0.8101\n",
      "Epoch 699/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.8006 - val_loss: 0.4150 - val_accuracy: 0.8156\n",
      "Epoch 700/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.8034 - val_loss: 0.4117 - val_accuracy: 0.8101\n",
      "Epoch 701/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7935 - val_loss: 0.4166 - val_accuracy: 0.8156\n",
      "Epoch 702/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.7978 - val_loss: 0.4129 - val_accuracy: 0.8268\n",
      "Epoch 703/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.8048 - val_loss: 0.4201 - val_accuracy: 0.8101\n",
      "Epoch 704/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7978 - val_loss: 0.4112 - val_accuracy: 0.8212\n",
      "Epoch 705/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7978 - val_loss: 0.4122 - val_accuracy: 0.8268\n",
      "Epoch 706/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.8006 - val_loss: 0.4152 - val_accuracy: 0.8268\n",
      "Epoch 707/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.8034 - val_loss: 0.4175 - val_accuracy: 0.8101\n",
      "Epoch 708/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7949 - val_loss: 0.4224 - val_accuracy: 0.8101\n",
      "Epoch 709/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.8062 - val_loss: 0.4139 - val_accuracy: 0.8156\n",
      "Epoch 710/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.8006 - val_loss: 0.4149 - val_accuracy: 0.8212\n",
      "Epoch 711/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.8020 - val_loss: 0.4151 - val_accuracy: 0.8324\n",
      "Epoch 712/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7992 - val_loss: 0.4155 - val_accuracy: 0.8156\n",
      "Epoch 713/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7992 - val_loss: 0.4130 - val_accuracy: 0.8268\n",
      "Epoch 714/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7992 - val_loss: 0.4108 - val_accuracy: 0.8101\n",
      "Epoch 715/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.8034 - val_loss: 0.4244 - val_accuracy: 0.8101\n",
      "Epoch 716/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8020 - val_loss: 0.4109 - val_accuracy: 0.8156\n",
      "Epoch 717/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.8062 - val_loss: 0.4255 - val_accuracy: 0.8101\n",
      "Epoch 718/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7978 - val_loss: 0.4166 - val_accuracy: 0.8324\n",
      "Epoch 719/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.8006 - val_loss: 0.4114 - val_accuracy: 0.8212\n",
      "Epoch 720/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4742 - accuracy: 0.8006 - val_loss: 0.4240 - val_accuracy: 0.8156\n",
      "Epoch 721/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7949 - val_loss: 0.4153 - val_accuracy: 0.8268\n",
      "Epoch 722/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7963 - val_loss: 0.4107 - val_accuracy: 0.8101\n",
      "Epoch 723/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.8034 - val_loss: 0.4133 - val_accuracy: 0.8212\n",
      "Epoch 724/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.7992 - val_loss: 0.4125 - val_accuracy: 0.8268\n",
      "Epoch 725/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7921 - val_loss: 0.4088 - val_accuracy: 0.8156\n",
      "Epoch 726/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7992 - val_loss: 0.4164 - val_accuracy: 0.8156\n",
      "Epoch 727/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8020 - val_loss: 0.4113 - val_accuracy: 0.8156\n",
      "Epoch 728/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.8048 - val_loss: 0.4133 - val_accuracy: 0.8156\n",
      "Epoch 729/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.8034 - val_loss: 0.4098 - val_accuracy: 0.8101\n",
      "Epoch 730/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7921 - val_loss: 0.4095 - val_accuracy: 0.8212\n",
      "Epoch 731/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7992 - val_loss: 0.4105 - val_accuracy: 0.8156\n",
      "Epoch 732/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.8006 - val_loss: 0.4100 - val_accuracy: 0.8212\n",
      "Epoch 733/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.8006 - val_loss: 0.4098 - val_accuracy: 0.8156\n",
      "Epoch 734/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4772 - accuracy: 0.8006 - val_loss: 0.4114 - val_accuracy: 0.8156\n",
      "Epoch 735/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.8020 - val_loss: 0.4106 - val_accuracy: 0.8212\n",
      "Epoch 736/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.7963 - val_loss: 0.4143 - val_accuracy: 0.8101\n",
      "Epoch 737/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7992 - val_loss: 0.4116 - val_accuracy: 0.8156\n",
      "Epoch 738/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.8020 - val_loss: 0.4110 - val_accuracy: 0.8101\n",
      "Epoch 739/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.8020 - val_loss: 0.4126 - val_accuracy: 0.8156\n",
      "Epoch 740/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7978 - val_loss: 0.4086 - val_accuracy: 0.8212\n",
      "Epoch 741/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7879 - val_loss: 0.4101 - val_accuracy: 0.8156\n",
      "Epoch 742/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.8034 - val_loss: 0.4117 - val_accuracy: 0.8268\n",
      "Epoch 743/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.8020 - val_loss: 0.4118 - val_accuracy: 0.8268\n",
      "Epoch 744/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.8020 - val_loss: 0.4111 - val_accuracy: 0.8212\n",
      "Epoch 745/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7963 - val_loss: 0.4092 - val_accuracy: 0.8156\n",
      "Epoch 746/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7949 - val_loss: 0.4087 - val_accuracy: 0.8212\n",
      "Epoch 747/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4736 - accuracy: 0.8048 - val_loss: 0.4194 - val_accuracy: 0.8101\n",
      "Epoch 748/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.8020 - val_loss: 0.4102 - val_accuracy: 0.8212\n",
      "Epoch 749/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.7992 - val_loss: 0.4115 - val_accuracy: 0.8156\n",
      "Epoch 750/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.8006 - val_loss: 0.4144 - val_accuracy: 0.8268\n",
      "Epoch 751/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7963 - val_loss: 0.4098 - val_accuracy: 0.8268\n",
      "Epoch 752/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7992 - val_loss: 0.4108 - val_accuracy: 0.8156\n",
      "Epoch 753/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8076 - val_loss: 0.4125 - val_accuracy: 0.8268\n",
      "Epoch 754/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.7992 - val_loss: 0.4091 - val_accuracy: 0.8268\n",
      "Epoch 755/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.8006 - val_loss: 0.4079 - val_accuracy: 0.8101\n",
      "Epoch 756/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4744 - accuracy: 0.7963 - val_loss: 0.4106 - val_accuracy: 0.8268\n",
      "Epoch 757/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.8006 - val_loss: 0.4131 - val_accuracy: 0.8101\n",
      "Epoch 758/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7978 - val_loss: 0.4088 - val_accuracy: 0.8268\n",
      "Epoch 759/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.7963 - val_loss: 0.4074 - val_accuracy: 0.8101\n",
      "Epoch 760/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.7978 - val_loss: 0.4232 - val_accuracy: 0.8101\n",
      "Epoch 761/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7935 - val_loss: 0.4117 - val_accuracy: 0.8268\n",
      "Epoch 762/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.8006 - val_loss: 0.4162 - val_accuracy: 0.8101\n",
      "Epoch 763/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4728 - accuracy: 0.8020 - val_loss: 0.4185 - val_accuracy: 0.8101\n",
      "Epoch 764/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7921 - val_loss: 0.4098 - val_accuracy: 0.8268\n",
      "Epoch 765/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7992 - val_loss: 0.4103 - val_accuracy: 0.8268\n",
      "Epoch 766/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.8006 - val_loss: 0.4115 - val_accuracy: 0.8212\n",
      "Epoch 767/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4733 - accuracy: 0.8048 - val_loss: 0.4102 - val_accuracy: 0.8212\n",
      "Epoch 768/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7992 - val_loss: 0.4158 - val_accuracy: 0.8156\n",
      "Epoch 769/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7978 - val_loss: 0.4117 - val_accuracy: 0.8212\n",
      "Epoch 770/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.8006 - val_loss: 0.4079 - val_accuracy: 0.8156\n",
      "Epoch 771/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4716 - accuracy: 0.7992 - val_loss: 0.4171 - val_accuracy: 0.8101\n",
      "Epoch 772/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7978 - val_loss: 0.4085 - val_accuracy: 0.8101\n",
      "Epoch 773/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7978 - val_loss: 0.4108 - val_accuracy: 0.8156\n",
      "Epoch 774/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.8034 - val_loss: 0.4133 - val_accuracy: 0.8156\n",
      "Epoch 775/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.7978 - val_loss: 0.4235 - val_accuracy: 0.8156\n",
      "Epoch 776/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.8034 - val_loss: 0.4153 - val_accuracy: 0.8156\n",
      "Epoch 777/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.8048 - val_loss: 0.4082 - val_accuracy: 0.8156\n",
      "Epoch 778/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.8034 - val_loss: 0.4119 - val_accuracy: 0.8268\n",
      "Epoch 779/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7963 - val_loss: 0.4084 - val_accuracy: 0.8212\n",
      "Epoch 780/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.8020 - val_loss: 0.4147 - val_accuracy: 0.8156\n",
      "Epoch 781/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.7992 - val_loss: 0.4142 - val_accuracy: 0.8101\n",
      "Epoch 782/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4755 - accuracy: 0.8034 - val_loss: 0.4140 - val_accuracy: 0.8156\n",
      "Epoch 783/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4733 - accuracy: 0.8006 - val_loss: 0.4089 - val_accuracy: 0.8268\n",
      "Epoch 784/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7992 - val_loss: 0.4104 - val_accuracy: 0.8212\n",
      "Epoch 785/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7992 - val_loss: 0.4056 - val_accuracy: 0.8101\n",
      "Epoch 786/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.8020 - val_loss: 0.4121 - val_accuracy: 0.8101\n",
      "Epoch 787/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.8006 - val_loss: 0.4085 - val_accuracy: 0.8212\n",
      "Epoch 788/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7935 - val_loss: 0.4071 - val_accuracy: 0.8212\n",
      "Epoch 789/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.7978 - val_loss: 0.4080 - val_accuracy: 0.8212\n",
      "Epoch 790/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.7935 - val_loss: 0.4121 - val_accuracy: 0.8156\n",
      "Epoch 791/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4720 - accuracy: 0.8006 - val_loss: 0.4047 - val_accuracy: 0.8101\n",
      "Epoch 792/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.8048 - val_loss: 0.4244 - val_accuracy: 0.8156\n",
      "Epoch 793/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4727 - accuracy: 0.7992 - val_loss: 0.4095 - val_accuracy: 0.8212\n",
      "Epoch 794/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4723 - accuracy: 0.7992 - val_loss: 0.4086 - val_accuracy: 0.8156\n",
      "Epoch 795/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4721 - accuracy: 0.8034 - val_loss: 0.4057 - val_accuracy: 0.8101\n",
      "Epoch 796/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.7978 - val_loss: 0.4101 - val_accuracy: 0.8156\n",
      "Epoch 797/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4718 - accuracy: 0.8006 - val_loss: 0.4077 - val_accuracy: 0.8212\n",
      "Epoch 798/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4737 - accuracy: 0.8048 - val_loss: 0.4071 - val_accuracy: 0.8212\n",
      "Epoch 799/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4715 - accuracy: 0.8006 - val_loss: 0.4079 - val_accuracy: 0.8212\n",
      "Epoch 800/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4730 - accuracy: 0.8048 - val_loss: 0.4059 - val_accuracy: 0.8156\n",
      "Epoch 801/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4720 - accuracy: 0.7949 - val_loss: 0.4108 - val_accuracy: 0.8156\n",
      "Epoch 802/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8006 - val_loss: 0.4109 - val_accuracy: 0.8156\n",
      "Epoch 803/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4745 - accuracy: 0.8006 - val_loss: 0.4058 - val_accuracy: 0.8156\n",
      "Epoch 804/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4725 - accuracy: 0.8076 - val_loss: 0.4073 - val_accuracy: 0.8101\n",
      "Epoch 805/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.8006 - val_loss: 0.4074 - val_accuracy: 0.8212\n",
      "Epoch 806/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.7949 - val_loss: 0.4107 - val_accuracy: 0.8156\n",
      "Epoch 807/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4711 - accuracy: 0.8020 - val_loss: 0.4148 - val_accuracy: 0.8101\n",
      "Epoch 808/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4731 - accuracy: 0.7935 - val_loss: 0.4095 - val_accuracy: 0.8156\n",
      "Epoch 809/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.8006 - val_loss: 0.4083 - val_accuracy: 0.8156\n",
      "Epoch 810/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4749 - accuracy: 0.7978 - val_loss: 0.4099 - val_accuracy: 0.8156\n",
      "Epoch 811/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.8006 - val_loss: 0.4074 - val_accuracy: 0.8156\n",
      "Epoch 812/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4713 - accuracy: 0.8034 - val_loss: 0.4059 - val_accuracy: 0.8156\n",
      "Epoch 813/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4732 - accuracy: 0.8048 - val_loss: 0.4119 - val_accuracy: 0.8156\n",
      "Epoch 814/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4715 - accuracy: 0.8020 - val_loss: 0.4066 - val_accuracy: 0.8101\n",
      "Epoch 815/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4719 - accuracy: 0.8034 - val_loss: 0.4100 - val_accuracy: 0.8212\n",
      "Epoch 816/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.7992 - val_loss: 0.4075 - val_accuracy: 0.8156\n",
      "Epoch 817/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.7978 - val_loss: 0.4062 - val_accuracy: 0.8156\n",
      "Epoch 818/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.8034 - val_loss: 0.4070 - val_accuracy: 0.8212\n",
      "Epoch 819/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4721 - accuracy: 0.7907 - val_loss: 0.4047 - val_accuracy: 0.8212\n",
      "Epoch 820/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4717 - accuracy: 0.8006 - val_loss: 0.4114 - val_accuracy: 0.8212\n",
      "Epoch 821/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4719 - accuracy: 0.7992 - val_loss: 0.4062 - val_accuracy: 0.8212\n",
      "Epoch 822/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4705 - accuracy: 0.8034 - val_loss: 0.4078 - val_accuracy: 0.8212\n",
      "Epoch 823/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4715 - accuracy: 0.8006 - val_loss: 0.4046 - val_accuracy: 0.8156\n",
      "Epoch 824/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4696 - accuracy: 0.8076 - val_loss: 0.4085 - val_accuracy: 0.8212\n",
      "Epoch 825/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4716 - accuracy: 0.7935 - val_loss: 0.4077 - val_accuracy: 0.8156\n",
      "Epoch 826/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4724 - accuracy: 0.7992 - val_loss: 0.4082 - val_accuracy: 0.8156\n",
      "Epoch 827/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4704 - accuracy: 0.7992 - val_loss: 0.4151 - val_accuracy: 0.8156\n",
      "Epoch 828/10000\n",
      "178/178 [==============================] - 1s 4ms/step - loss: 0.4687 - accuracy: 0.8020 - val_loss: 0.4082 - val_accuracy: 0.8156\n",
      "Epoch 829/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4721 - accuracy: 0.7978 - val_loss: 0.4084 - val_accuracy: 0.8212\n",
      "Epoch 830/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4720 - accuracy: 0.8076 - val_loss: 0.4033 - val_accuracy: 0.8045\n",
      "Epoch 831/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4705 - accuracy: 0.8048 - val_loss: 0.4083 - val_accuracy: 0.8101\n",
      "Epoch 832/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4698 - accuracy: 0.7935 - val_loss: 0.4042 - val_accuracy: 0.8212\n",
      "Epoch 833/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4692 - accuracy: 0.8034 - val_loss: 0.4049 - val_accuracy: 0.8101\n",
      "Epoch 834/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4716 - accuracy: 0.8048 - val_loss: 0.4081 - val_accuracy: 0.8212\n",
      "Epoch 835/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4704 - accuracy: 0.7992 - val_loss: 0.4062 - val_accuracy: 0.8212\n",
      "Epoch 836/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4703 - accuracy: 0.8020 - val_loss: 0.4077 - val_accuracy: 0.8212\n",
      "Epoch 837/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4708 - accuracy: 0.8048 - val_loss: 0.4140 - val_accuracy: 0.8156\n",
      "Epoch 838/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.7978 - val_loss: 0.4042 - val_accuracy: 0.8212\n",
      "Epoch 839/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4703 - accuracy: 0.7992 - val_loss: 0.4093 - val_accuracy: 0.8156\n",
      "Epoch 840/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.8006 - val_loss: 0.4089 - val_accuracy: 0.8156\n",
      "Epoch 841/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.8034 - val_loss: 0.4064 - val_accuracy: 0.8212\n",
      "Epoch 842/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.8034 - val_loss: 0.4041 - val_accuracy: 0.8156\n",
      "Epoch 843/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4694 - accuracy: 0.8048 - val_loss: 0.4079 - val_accuracy: 0.8156\n",
      "Epoch 844/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.8020 - val_loss: 0.4110 - val_accuracy: 0.8156\n",
      "Epoch 845/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4704 - accuracy: 0.7992 - val_loss: 0.4056 - val_accuracy: 0.8156\n",
      "Epoch 846/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4703 - accuracy: 0.8034 - val_loss: 0.4098 - val_accuracy: 0.8156\n",
      "Epoch 847/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4678 - accuracy: 0.8076 - val_loss: 0.4104 - val_accuracy: 0.8156\n",
      "Epoch 848/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4698 - accuracy: 0.7893 - val_loss: 0.4132 - val_accuracy: 0.8101\n",
      "Epoch 849/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7978 - val_loss: 0.4060 - val_accuracy: 0.8156\n",
      "Epoch 850/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4666 - accuracy: 0.8062 - val_loss: 0.4101 - val_accuracy: 0.8101\n",
      "Epoch 851/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4716 - accuracy: 0.8062 - val_loss: 0.4074 - val_accuracy: 0.8156\n",
      "Epoch 852/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7978 - val_loss: 0.4112 - val_accuracy: 0.8212\n",
      "Epoch 853/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.8034 - val_loss: 0.4077 - val_accuracy: 0.8156\n",
      "Epoch 854/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.7935 - val_loss: 0.4068 - val_accuracy: 0.8212\n",
      "Epoch 855/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4694 - accuracy: 0.7949 - val_loss: 0.4096 - val_accuracy: 0.8212\n",
      "Epoch 856/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.8020 - val_loss: 0.4085 - val_accuracy: 0.8156\n",
      "Epoch 857/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4672 - accuracy: 0.7963 - val_loss: 0.4209 - val_accuracy: 0.8156\n",
      "Epoch 858/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4697 - accuracy: 0.8090 - val_loss: 0.4139 - val_accuracy: 0.8156\n",
      "Epoch 859/10000\n",
      "178/178 [==============================] - 0s 3ms/step - loss: 0.4689 - accuracy: 0.7992 - val_loss: 0.4056 - val_accuracy: 0.8156\n",
      "Epoch 860/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.8020 - val_loss: 0.4102 - val_accuracy: 0.8212\n",
      "Epoch 861/10000\n",
      "178/178 [==============================] - 1s 3ms/step - loss: 0.4693 - accuracy: 0.7978 - val_loss: 0.4044 - val_accuracy: 0.8156\n",
      "Epoch 862/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7992 - val_loss: 0.4068 - val_accuracy: 0.8156\n",
      "Epoch 863/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.8020 - val_loss: 0.4089 - val_accuracy: 0.8156\n",
      "Epoch 864/10000\n",
      "178/178 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.8020 - val_loss: 0.4133 - val_accuracy: 0.8212\n",
      "Epoch 865/10000\n",
      " 43/178 [======>.......................] - ETA: 0s - loss: 0.4736 - accuracy: 0.8081"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# if loss is not improving within 10 epochs, stop and get the best one\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m early_stop \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m ,validation_split \u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[early_stop])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "reset_to_initial_weights(model, initial_weights)\n",
    "\n",
    "# if loss is not improving within 10 epochs, stop and get the best one\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=150, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=4 ,validation_split = 0.2, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = history.history['loss']\n",
    "accuracy = history.history['accuracy']\n",
    "plt.figure(figsize=(8, 4));\n",
    "to_plot = [history.history['loss'], history.history['val_loss'], history.history['accuracy'], history.history['val_accuracy']]\n",
    "label = ['Training Loss','Validation Loss','Training Accuracy','Validation Accuracy']\n",
    "colors = ['red','orange','green', 'darkcyan']\n",
    "style = ['-','--','-','--']\n",
    "\n",
    "\n",
    "for i,j,k,l in zip(to_plot, colors, label, style):\n",
    "    plt.plot(i, label=k, color=j, linestyle=l, alpha=0.85)\n",
    "plt.legend(bbox_to_anchor=(0.5,1.08), loc='center', frameon=False, ncol=4, prop={'size': 10})\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "threshold = np.linspace(0.5,0.75,20)\n",
    "yhat = model.predict(X_train)\n",
    "yhat = tf.math.sigmoid(yhat)\n",
    "thresh_list = []\n",
    "accuracy_list = []\n",
    "for i in threshold:\n",
    "    yhat = np.where(yhat>=i, 1, 0)\n",
    "    yhat_series = pd.Series(yhat.ravel())\n",
    "    yhat_series.index = y_train.index\n",
    "    yhat_series.name = y_train.name\n",
    "    accuracy = (y_train == yhat_series).mean()\n",
    "    thresh_list.append(i)\n",
    "    accuracy_list.append(accuracy)\n",
    "best_threshold = thresh_list[np.argmax(accuracy_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles found are ['Mr' 'Mrs' 'Miss' 'Master' 'Ms' 'Col' 'Rev' 'Dr' 'Dona']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Survived'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Survived'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y101sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m clean_test_data \u001b[39m=\u001b[39m drop_irrev_col(test_data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y101sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m clean_test_data \u001b[39m=\u001b[39m feature_engineering(clean_test_data)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y101sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_test,_ \u001b[39m=\u001b[39m features_and_target(clean_test_data)\n",
      "\u001b[1;32m/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y101sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m X \u001b[39m=\u001b[39m df [features]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y101sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# binary target\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y101sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m Y \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mSurvived\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y101sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, Y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Survived'"
     ]
    }
   ],
   "source": [
    "clean_test_data['Survived']=0\n",
    "clean_test_data = drop_irrev_col(test_data)\n",
    "clean_test_data = feature_engineering(clean_test_data)\n",
    "X_test,_ = features_and_target(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_9/L_i/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/omar/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/omar/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_29496/4109673188.py\", line 1, in <module>\n      predictions = model.predict(X_test)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2554, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2341, in predict_function\n      return step_function(self, iterator)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2327, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2315, in run_step\n      outputs = model.predict_step(data)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n      return self(x, training=False)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py\", line 255, in call\n      outputs = self.activation(outputs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_9/L_i/Relu'\nMatrix size-incompatible: In[0]: [32,16], In[1]: [24,16]\n\t [[{{node sequential_9/L_i/Relu}}]] [Op:__inference_predict_function_5967410]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/omar/data_science_ML/projects/1_titanic/model_predict.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_9/L_i/Relu' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/omar/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/home/omar/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/omar/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/omar/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_29496/4109673188.py\", line 1, in <module>\n      predictions = model.predict(X_test)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2554, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2341, in predict_function\n      return step_function(self, iterator)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2327, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2315, in run_step\n      outputs = model.predict_step(data)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2283, in predict_step\n      return self(x, training=False)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 569, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/sequential.py\", line 405, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/engine/base_layer.py\", line 1150, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py\", line 255, in call\n      outputs = self.activation(outputs)\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/activations.py\", line 321, in relu\n      return backend.relu(\n    File \"/home/omar/.local/lib/python3.10/site-packages/keras/src/backend.py\", line 5397, in relu\n      x = tf.nn.relu(x)\nNode: 'sequential_9/L_i/Relu'\nMatrix size-incompatible: In[0]: [32,16], In[1]: [24,16]\n\t [[{{node sequential_9/L_i/Relu}}]] [Op:__inference_predict_function_5967410]"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 24)\n",
      "(418, 16)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
